{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "import code\n",
    "import numpy as np\n",
    "import ntpath\n",
    "from pathlib import Path\n",
    "import os\n",
    "from baseline_model import * \n",
    "#from particle import Particle_Filter \n",
    "import pandas as pd\n",
    "import math\n",
    "#from filterpy.monte_carlo import systematic_resample\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = Path('./../../..')\n",
    "\n",
    "config = {}\n",
    "config['model'] = root_path / 'baboon_tracking' / 'models' / 'particle_filter' / 'net.pth'\n",
    "config['input_dim'] = 9\n",
    "config['output_dim'] = -1 \n",
    "config['kmeans_model_path'] = root_path / 'ml_data' / 'velocity_model.pkl'\n",
    "config['input_csv'] = root_path / 'ml_data' / 'input_mp4.csv'\n",
    "config['video_csv'] = root_path / 'ml_data' / '4_22_2020_mask.mp4_blobdetector.csv'\n",
    "\n",
    "########## csv Initialization ##########\n",
    "data = pd.read_csv(config['input_csv'])\n",
    "video = pd.read_csv(config['video_csv'])\n",
    "del video['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Particle_Filter():\n",
    "    def __init__(self, estimated_location, init_velocities, weights, estimated_velocity, direction_vector, des_num_particles):\n",
    "        # velocities\n",
    "        self.velocities = init_velocities\n",
    "        self.weights = weights\n",
    "        \n",
    "        # estimated location of baboon at initialization\n",
    "        self.estimated_location = estimated_location\n",
    "        self.estimated_velocity = estimated_velocity\n",
    "        self.direction_vector = direction_vector\n",
    "        self.des_num_particles = des_num_particles\n",
    "                \n",
    "    #frame - n x 2 matrix of all blob detected centroids in the current frame\n",
    "    #threshold - pixel distance from the previous position that points should be considered from the blob detector\n",
    "    #alpha - degree of trust in blob detector (probably 0.8-0.9)\n",
    "    def update(self, frame, threshold, alpha=0.8, noise=8, FPS=30):\n",
    "        predict_velocities = self.velocities\n",
    "        predict_weights = self.weights\n",
    "        #print(\"Starting velocity\")\n",
    "        #print(predict_velocities)\n",
    "        #print(\"Starting weights\")\n",
    "        #print(predict_weights)\n",
    "        prev_position = self.estimated_location\n",
    "        prev_velocity = self.estimated_velocity\n",
    "        new_velocities = []\n",
    "        new_weights = []\n",
    "        \n",
    "        #convert velocity from predict into coordinate\n",
    "        particle_cord = predict_velocities.T * direction_vector + prev_position\n",
    "        #print(\"particle_cord\")\n",
    "        #print(particle_cord)\n",
    "        #get all blobs within threshold range\n",
    "        euclidian_distances = np.sqrt(np.square(frame[['x']]-prev_position[0][0])['x'] + np.square(frame[['y']]-prev_position[0][1])['y'] )\n",
    "        nearest_blobs = frame[euclidian_distances < threshold]\n",
    "        \n",
    "        #if there are no nearby blobs, exit update\n",
    "        if nearest_blobs.shape == (0,1):\n",
    "            #print(\"no blobs nearby!\")\n",
    "            return\n",
    "        \n",
    "        #if there is a single blob nearby\n",
    "        elif nearest_blobs.shape[0] == 1:\n",
    "            #print(\"one blob nearby\")\n",
    "            nearest_blobs = np.full(particle_cord.shape, nearest_blobs)\n",
    "            #print(nearest_blobs)\n",
    "            \n",
    "        \n",
    "        #if there are multiple blobs nearby, pick the best one by closest distance\n",
    "        else:\n",
    "            #print(\"more than one nearby blob\")\n",
    "            best_blob = np.zeros(particle_cord.shape)\n",
    "            best_dist = np.full((particle_cord.shape[0],1), np.Infinity)\n",
    "            for i, blob in nearest_blobs.iterrows():\n",
    "                #print(\"blob #\", i,\": \", blob, blob.shape)\n",
    "                dist = np.sqrt( np.square(np.full((particle_cord.shape[0]),blob[0])-particle_cord[:,0]) + np.square(np.full((particle_cord.shape[0]),blob[1])-particle_cord[:,1]))\n",
    "                dist = np.array([dist]).T\n",
    "                #print(\"dist: \", dist)\n",
    "                best_blob[np.squeeze([dist < best_dist]),:] = blob\n",
    "                #print(\"bestblob: \", best_blob)\n",
    "                best_dist[dist < best_dist] = dist[dist < best_dist]\n",
    "                #print(\"best_dist: \", best_dist)\n",
    "            #print(nearest_blobs)\n",
    "            nearest_blobs = best_blob\n",
    "            \n",
    "        nearest_blobs = pd.DataFrame(data=nearest_blobs, columns=[\"x\", \"y\"])\n",
    "            \n",
    "        #update coordinates for each particle & blob\n",
    "        \n",
    "        #print(\"particle_cord\")\n",
    "        #print(particle_cord)\n",
    "        #print(particle_cord.shape)\n",
    "        #updated coordinate = (blobs + normalized random values * 0.8) + (predicted coordinates * 0.2)\n",
    "        updated_cord = (nearest_blobs+np.random.standard_normal(nearest_blobs.shape)*noise)*alpha + particle_cord*(1-alpha)\n",
    "        #print(\"Updated Cord\")\n",
    "        #print(updated_cord)\n",
    "            \n",
    "        #Calculate velocity for new coordinate\n",
    "        #CHANGE THIS: change prev_position to blob_position\n",
    "        new_dist = np.sqrt(np.square(updated_cord[['x']]-prev_position[0][0])['x'] + np.square(updated_cord[['y']]-prev_position[0][1])['y'] )\n",
    "        new_velocities.append(new_dist/FPS)\n",
    "        #print(\"new_dist: \", new_dist)\n",
    "        #print(predict_weights.shape)\n",
    "        #print(new_dist.shape)\n",
    "        blob_dist = np.sqrt(np.square(updated_cord[['x']]-nearest_blobs[['x']])['x'] + np.square(updated_cord[['y']]-nearest_blobs[['y']])['y'] )\n",
    "        #print(\"blob dist\")\n",
    "        #print(blob_dist)\n",
    "        new_weights = (max(blob_dist)-blob_dist)/max(blob_dist)*alpha+(predict_weights.squeeze()*(1-alpha))\n",
    "        new_weights = new_weights/sum(new_weights)\n",
    "\n",
    "        #print(\"Ending velocities\")\n",
    "        #print(new_velocities)\n",
    "        #print(\"Ending weights\")\n",
    "        #print(new_weights)\n",
    "        self.velocities = np.array(new_velocities)\n",
    "        self.weights = np.array(new_weights)\n",
    "    \n",
    "    def resample(self):\n",
    "        \n",
    "        # Define variables \n",
    "        velocities = self.velocities\n",
    "        weights = self.weights\n",
    "        des_num_particles = self.des_num_particles\n",
    "        threshold = 1/des_num_particles\n",
    "        \n",
    "        # 1) Normalize the weights first (divide each weight by the sum of all the weights) \n",
    "        weights = np.divide(weights, np.sum(weights))\n",
    "#       print(\"Normalized weights\")\n",
    "#       print(weights)\n",
    "\n",
    "        # 2) Removing particles below threshold (< 1/N)\n",
    "        if len(weights) <= des_num_particles: \n",
    "            print(weights)\n",
    "            print(velocities)\n",
    "            \n",
    "            velocities = velocities.flatten()\n",
    "            velocities = velocities[weights > 1/des_num_particles]\n",
    "            boolean_mask = weights > 1/des_num_particles\n",
    "            print(\"Boolean\")\n",
    "            print(boolean_mask)\n",
    "            weights = weights[boolean_mask]\n",
    "#         print(\"Removes particles below threshold (1/N)\")\n",
    "#         print(weights)\n",
    "\n",
    "        # 3) Truncate (if greater than N). \n",
    "        # Ensure that we have our desired N particles, remove the lowest \n",
    "        if len(weights) > des_num_particles: \n",
    "            num = des_num_particles\n",
    "            diff = len(weights) - des_num_particles\n",
    "\n",
    "            for i in range(diff): \n",
    "                index = np.argmin(weights)\n",
    "                np.delete(weights, index)\n",
    "                np.delete(weights, index)\n",
    "                \n",
    "#         print(\"If greater than N, then we remove the lowest weight values\")\n",
    "#         print(weights) \n",
    "#         print(velocities)\n",
    "\n",
    "        # 4) Normalize the weights that remain --> we have numbers that represent what portion of N that particular velocity has\n",
    "        weights = np.divide(weights, np.sum(weights))\n",
    "#         print(\"Normalizes the weights that remain\")\n",
    "#         print(weights)\n",
    "\n",
    "        ### Shifting: if the number of particles is less than N...###\n",
    "        # 5) Multiply the weights by the desired number of particles then take round\n",
    "        # to get the number of particles we want representing those velocities and weights \n",
    "\n",
    "        # weights = weights*des_num_particles \n",
    "#         print(\"Proportion\")\n",
    "#         print(weights*des_num_particles)\n",
    "#         print(\"Weights\")\n",
    "#         print(weights)\n",
    "        proportion_particles = np.round_(weights*des_num_particles, decimals = 0, out=None)\n",
    "        proportion_particles = list(proportion_particles.flatten())\n",
    "#         print(\"Proportion of particles\")\n",
    "#         print(proportion_particles) #should output number of particles to replicate\n",
    "\n",
    "        # 6) Duplicate the number of particles\n",
    "        # TO DO: initialize new_weights?\n",
    "        # at each index, replicate weight and particle at that many times\n",
    "\n",
    "        updated_weights = np.repeat(weights, proportion_particles)\n",
    "        updated_velocities = np.repeat(velocities, proportion_particles)\n",
    "#         print(\"Duplicated particles\")\n",
    "#         print(\"Weights\")\n",
    "#         print(updated_weights)\n",
    "#         print(\"Velocities\")\n",
    "#         print(updated_velocities)\n",
    "\n",
    "        weights = updated_weights\n",
    "        velocities = updated_velocities\n",
    "\n",
    "        # 7) Set all the weights so that the weights are 1/total number of particles\n",
    "        weights = np.full(weights.shape, 1/weights.size)\n",
    "        #print(\"Final weights\")\n",
    "        #print(weights)\n",
    "        assert np.sum(weights) == 1 # check that weights sum to 1          \n",
    "        \n",
    "        # Update velocities and weights\n",
    "        self.weights = updated_weights\n",
    "        self.velocities = updated_velocities\n",
    "        \n",
    "        #print(\"Updated velocity from resample function\")\n",
    "        #print(self.velocities)\n",
    "        #print(\"Updated weights from resample function\")\n",
    "        #print(self.weights)\n",
    "    \n",
    "    def estimate(self):\n",
    "        estimated_velocity = np.average(self.velocities, axis=1, weights=self.weights)\n",
    "        self.estimated_velocity = estimated_velocity\n",
    "        print(\"Print estimated velocity\")\n",
    "        print(self.estimated_velocity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "velocities = np.array([[0.0, 1.0, 2.0, 3.0, 4.0, 5.0]])\n",
    "weights = np.array([[0.1, 0.3, 0.1, 0.1, 0.1, 0.4]])\n",
    "prev_position = np.array([[2800,1600]])\n",
    "prev_velocity = 5.0\n",
    "direction_vector = np.array([[0.3, 0.5]])\n",
    "estimated_location = np.array([[3400, 1400]])\n",
    "threshold = 100\n",
    "alpha = 0.8\n",
    "noise = 8\n",
    "des_num_particles = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.12084535 0.28264999 0.07499065 0.08285532 0.12264248 0.31601621]\n",
      "[[97.97315225 98.06991164 97.59065811 97.6583198  97.98664889 97.72366087]]\n",
      "Boolean\n",
      "[False  True False False False  True]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (8,) (1,2) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-204-e2f289255981>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# then in for each baboon/particle filter (call predict, update, resample)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mparticle_filter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthisFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mparticle_filter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# particle_filter.estimate()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-202-e47a343438ce>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, frame, threshold, alpha, noise, FPS)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m#convert velocity from predict into coordinate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mparticle_cord\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_velocities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdirection_vector\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mprev_position\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0;31m#print(\"particle_cord\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m#print(particle_cord)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (8,) (1,2) "
     ]
    }
   ],
   "source": [
    "particle_filter = Particle_Filter(estimated_location, velocities, weights, prev_velocity, direction_vector, des_num_particles)\n",
    "# num_particles = desired number of particles\n",
    "#   def __init__(self, estimated_location, init_velocities, weights, estimated_velocity, direction_vector):\n",
    "\n",
    "all_frames = video['frame'].unique()\n",
    "for num in all_frames:\n",
    "    thisFrame = video.loc[video['frame'] == num]\n",
    "    \n",
    "    # for each frame, loop through each particle filter (one praticle filter per baboon)\n",
    "    # then in for each baboon/particle filter (call predict, update, resample)\n",
    "    \n",
    "    particle_filter.update(thisFrame.iloc[:, 1:3], threshold, alpha, noise)\n",
    "    particle_filter.resample()\n",
    "    # particle_filter.estimate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "velocities = np.array([[0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 0.8, 0.9]])\n",
    "weights = np.array([[0.03, 0.4, 0.1, 0.5, 0.1, 0.2, 0.1, 0.5, 0.8, 0.9]]) \n",
    "print(velocities.size)\n",
    "print(weights.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the original weight\n",
      "[[0.1 0.3 0.1 0.1 0.1 0.4]]\n",
      "Normalized weights\n",
      "[[0.09090909 0.27272727 0.09090909 0.09090909 0.09090909 0.36363636]]\n",
      "Removes particles below threshold (1/N)\n",
      "[0.27272727 0.36363636]\n",
      "If greater than N, then we remove the lowest weight values\n",
      "[0.27272727 0.36363636]\n",
      "[1. 5.]\n",
      "Normalizes the weights that remain\n",
      "[0.42857143 0.57142857]\n",
      "Proportion\n",
      "[2.14285714 2.85714286]\n",
      "Weights\n",
      "[0.42857143 0.57142857]\n",
      "Proportion of particles\n",
      "[2.0, 3.0]\n",
      "Duplicated particles\n",
      "Weights\n",
      "[0.42857143 0.42857143 0.57142857 0.57142857 0.57142857]\n",
      "Velocities\n",
      "[1. 1. 5. 5. 5.]\n",
      "Final weights\n",
      "[0.2 0.2 0.2 0.2 0.2]\n"
     ]
    }
   ],
   "source": [
    "# TESTING BUBBLE\n",
    "\n",
    "velocities = np.array([[0.0, 1.0, 2.0, 3.0, 4.0, 5.0]])\n",
    "weights = np.array([[0.1, 0.3, 0.1, 0.1, 0.1, 0.4]])\n",
    "des_num_particles = 5\n",
    "threshold = 1/des_num_particles\n",
    "\n",
    "print(\"This is the original weight\")\n",
    "print(weights)\n",
    "        \n",
    "# 1) Normalize the weights first (divide each weight by the sum of all the weights) \n",
    "weights = np.divide(weights, np.sum(weights))\n",
    "print(\"Normalized weights\")\n",
    "print(weights)\n",
    "\n",
    "# 2) Removing particles below threshold (< 1/N)\n",
    "if len(weights) <= des_num_particles: \n",
    "    velocities = velocities[weights > 1/des_num_particles]\n",
    "    boolean_mask = weights > 1/des_num_particles\n",
    "    weights = weights[boolean_mask]\n",
    "print(\"Removes particles below threshold (1/N)\")\n",
    "print(weights)\n",
    "\n",
    "# 3) Truncate (if greater than N). \n",
    "# Ensure that we have our desired N particles, remove the lowest \n",
    "if len(weights) > des_num_particles: \n",
    "    num = des_num_particles\n",
    "    diff = len(weights) - des_num_particles\n",
    "\n",
    "    for i in range(diff): \n",
    "        index = np.argmin(weights)\n",
    "        np.delete(weights, index)\n",
    "        np.delete(weights, index)\n",
    "print(\"If greater than N, then we remove the lowest weight values\")\n",
    "print(weights) \n",
    "print(velocities)\n",
    "\n",
    "# 4) Normalize the weights that remain --> we have numbers that represent what portion of N that particular velocity has\n",
    "weights = np.divide(weights, np.sum(weights))\n",
    "print(\"Normalizes the weights that remain\")\n",
    "print(weights)\n",
    "\n",
    "### Shifting: if the number of particles is less than N...###\n",
    "# 5) Multiply the weights by the desired number of particles then take round\n",
    "# to get the number of particles we want representing those velocities and weights \n",
    "\n",
    "# weights = weights*des_num_particles \n",
    "print(\"Proportion\")\n",
    "print(weights*des_num_particles)\n",
    "print(\"Weights\")\n",
    "print(weights)\n",
    "proportion_particles = np.round_(weights*des_num_particles, decimals = 0, out=None)\n",
    "proportion_particles = list(proportion_particles.flatten())\n",
    "print(\"Proportion of particles\")\n",
    "print(proportion_particles) #should output number of particles to replicate\n",
    "\n",
    "# 6) Duplicate the number of particles\n",
    "# TO DO: initialize new_weights?\n",
    "# at each index, replicate weight and particle at that many times\n",
    "\n",
    "updated_weights = np.repeat(weights, proportion_particles)\n",
    "updated_velocities = np.repeat(velocities, proportion_particles)\n",
    "print(\"Duplicated particles\")\n",
    "print(\"Weights\")\n",
    "print(updated_weights)\n",
    "print(\"Velocities\")\n",
    "print(updated_velocities)\n",
    "\n",
    "weights = updated_weights\n",
    "velocities = updated_velocities\n",
    "\n",
    "# 7) Set all the weights so that the weights are 1/total number of particles\n",
    "weights = np.full(weights.shape, 1/weights.size)\n",
    "print(\"Final weights\")\n",
    "print(weights)\n",
    "assert np.sum(weights) == 1 # check that weights sum to 1        \n",
    "\n",
    "#print(\"Updated velocity from resample function\")\n",
    "#print(self.velocities)\n",
    "#print(\"Updated weights from resample function\")\n",
    "#print(self.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
