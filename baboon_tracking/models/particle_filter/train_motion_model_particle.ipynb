{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please read the TODOs as they may be crucial depending on the dataset you are using to train. Configs are not specific to a dataset so choose those as you see appropriate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from baseline_model import *\n",
    "from pathlib import Path \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Path('../../..')\n",
    "\n",
    "config = {}\n",
    "config['k_nearest_baboons'] = 4 #getting the four nearest baboons per frame to consider(MUST BE EVEN)\n",
    "config['k_historic_velocities'] = 4 #getting the four past velocities to consider\n",
    "config['checkpoint_training'] = False\n",
    "config['saved_path'] = 'saved_weights.pth' #path to save current model information (epoch, weights, etc...)\n",
    "config['learning_rate'] = 0.0001 #lr\n",
    "config['batch_size'] = 32 # Number of training samples per batch to be passed to network\n",
    "config['val_split'] = 0.2 #ratio of val and training data\n",
    "config['shuffle_dataset'] = True #shuffle data before start (TODO: change this if we use kfold?)\n",
    "config['epochs'] = 100  # Number of epochs to train the model\n",
    "config['early_stop'] = True  # Implement early stopping or not\n",
    "config['early_stop_epoch'] = 3 # Number of epochs for which validation loss increases to be counted as overfitting\n",
    "config['input_training_data'] = root / 'output/DJI_0870_velocity.csv'\n",
    "config['kmeans_model_path'] = root / 'output' / 'velocity_model.pkl'\n",
    "\n",
    "\n",
    "#DO NOT CHANGE THE FOLLOWING \n",
    "config['input_dimension'] = config['k_nearest_baboons'] + config['k_historic_velocities'] + 1 #DON'T TOUCH THIS\n",
    "config['validation_loss_path'] = '' #leave empty \n",
    "config['training_loss_path'] = '' #leave empty \n",
    "config['output_dimension'] = -1 #leave empty\n",
    "\n",
    "#config checks\n",
    "assert( config['k_nearest_baboons'] % 2 == 0 )\n",
    "assert( os.path.isfile(config['input_training_data']) == True)\n",
    "assert( os.path.isfile(config['kmeans_model_path']) == True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize table for getting nearest baboons and their respective velocities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((2125.4, 1057.18), 51.074506175125435, 10.0),\n",
       " ((2053.7, 939.645), 0.44955044955004064, 11.0),\n",
       " ((2045.13, 905.955), 13.382885251805144, 12.0),\n",
       " ((2007.215, 900.905), 0.6178479958454139, 13.0),\n",
       " ((2161.0550000000007, 941.54), 3.7101154055304586, 14.0),\n",
       " ((2278.565, 1158.07), 6.985331880593899, 15.0),\n",
       " ((2314.695, 1177.34), 12.901044905119257, 16.0),\n",
       " ((2119.07, 809.89), 19.39097995929645, 17.0),\n",
       " ((1908.015, 763.5), 1.8776169959325903, 18.0),\n",
       " ((1923.885, 781.475), 6.434838327744581, 19.0),\n",
       " ((1950.245, 810.35), 1.6483516483603706, 20.0),\n",
       " ((1948.19, 826.1949999999998), 2.1455276318607357, 21.0),\n",
       " ((2105.445, 1237.58), 1.2081305317123932, 22.0),\n",
       " ((2124.105, 1239.65), 0.9477355624850088, 23.0),\n",
       " ((2490.31, 1190.72), 3.146853146853692, 24.0),\n",
       " ((2487.08, 1277.885), 1.0805847978347027, 25.0),\n",
       " ((1707.42, 1122.74), 0.14985014984660636, 26.0),\n",
       " ((1785.4699999999998, 1085.1), 15.050804630763162, 27.0),\n",
       " ((1800.63, 1090.315), 3.8526354042502913, 28.0),\n",
       " ((1865.715, 1096.415), 2.5781194606482063, 29.0),\n",
       " ((1936.615, 1130.0749999999996), 2.4393837057189582, 30.0),\n",
       " ((1836.705, 1101.535), 0.9477355624957832, 31.0),\n",
       " ((1514.1950000000004, 1070.125), 14.424585076251118, 32.0),\n",
       " ((1708.935, 1301.365), 1.0596005712115035, 33.0),\n",
       " ((2106.57, 1522.23), 21.45998549660368, 34.0),\n",
       " ((2046.85, 1537.89), 5.310702841163275, 35.0),\n",
       " ((2721.89, 1494.69), 0.6701502430032729, 36.0),\n",
       " ((2202.05, 1108.05), 0.2997002997068415, 37.0),\n",
       " ((2227.535, 1120.34), 0.9115028766610744, 38.0),\n",
       " ((2031.965, 816.7249999999998), 1.5281776764053896, 39.0),\n",
       " ((2057.76, 848.5), 1.5644814948393388, 40.0),\n",
       " ((2080.02, 859.9449999999998), 1.613935506635413, 41.0)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This is assuming the obvious assumption that a baboon only has one position per frame\n",
    "#TODO : Assuming data is ordered by frame (increasing timesteps/frames)\n",
    "training_data = pd.read_csv(config['input_training_data'])\n",
    "\n",
    "frames_to_velocities = {} #Maps a single frame to a list of all the baboons, their id, position, and velocity\n",
    "\n",
    "for idx, row in training_data.iterrows():\n",
    "    current_frame = row.loc['frame']\n",
    "    current_position = (row.loc['centroid_x'], row.loc['centroid_y'])\n",
    "    current_velocity = row.loc['velocity']\n",
    "    current_id = row.loc['baboon id']\n",
    "    \n",
    "    if current_frame not in frames_to_velocities:\n",
    "        frames_to_velocities[current_frame] = []\n",
    "    \n",
    "    frames_to_velocities[current_frame].append((current_position, current_velocity, current_id))\n",
    "\n",
    "\"\"\"\n",
    "Returns the K physically nearest baboons to baboon_id at the given frame\n",
    "\n",
    "Pre-pends with negative ones if there are less than k nearest baboons in the frame\n",
    "\"\"\"\n",
    "def get_k_nearest_baboons_velocities(frame, baboon_id):\n",
    "    baboons_in_frame = frames_to_velocities[frame].copy()\n",
    "    output = []\n",
    "    \n",
    "    target_position = (-1, -1)\n",
    "    \n",
    "    #get rid of the baboon we are considering\n",
    "    for idx, baboon in enumerate(baboons_in_frame):\n",
    "        if baboon[2] == baboon_id:\n",
    "            target_velocity = baboons_in_frame[idx][0]\n",
    "            del baboons_in_frame[idx]\n",
    "            break\n",
    "      \n",
    "    if len(baboons_in_frame) <= config['k_nearest_baboons']:\n",
    "        padded_output = list(np.ones(config['k_nearest_baboons'] - len(baboons_in_frame)) * -1)\n",
    "        return padded_output.extend([item[1] for item in baboons_in_frame])\n",
    "    \n",
    "    for k_nearest_neighbor in range(config['k_nearest_baboons']):\n",
    "        min_distance = math.inf\n",
    "        min_velocity = -1\n",
    "        min_idx = -1\n",
    "        \n",
    "        for idx, baboon in enumerate(baboons_in_frame):\n",
    "            current_distance = np.linalg.norm(np.array(baboon[0]) - np.array(target_velocity))\n",
    "            if current_distance < min_distance:\n",
    "                min_distance = current_distance\n",
    "                min_velocity = baboon[1]\n",
    "                min_idx = idx\n",
    "        \n",
    "        output.append(min_velocity)\n",
    "        del baboons_in_frame[min_idx]\n",
    "        \n",
    "    return output\n",
    "            \n",
    "frames_to_velocities[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.2997002997068415,\n",
       " 0.9115028766610744,\n",
       " 3.7101154055304586,\n",
       " 0.44955044955004064]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_k_nearest_baboons_velocities(1, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To get the historic velocities per baboon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(12.85046947525376, 21.0),\n",
       " (12.97826878451604, 22.0),\n",
       " (12.644369496333105, 23.0),\n",
       " (12.850469475230561, 24.0),\n",
       " (12.85046947525376, 25.0),\n",
       " (12.85046947522877, 26.0),\n",
       " (12.850469475230561, 27.0),\n",
       " (12.850469475242162, 28.0),\n",
       " (12.978268784517812, 29.0),\n",
       " (12.723151376903115, 30.0),\n",
       " (12.85046947522877, 31.0),\n",
       " (12.978268784519587, 32.0),\n",
       " (12.850469475240375, 33.0),\n",
       " (12.723151376903115, 34.0),\n",
       " (12.978268784517812, 35.0),\n",
       " (12.85046947522877, 36.0),\n",
       " (12.723151376891556, 37.0),\n",
       " (12.85046947525376, 38.0),\n",
       " (12.97826878451604, 39.0),\n",
       " (12.850469475232348, 40.0),\n",
       " (12.723151376903115, 41.0),\n",
       " (12.978268784517812, 42.0),\n",
       " (12.850469475240375, 43.0),\n",
       " (12.850469475230561, 44.0),\n",
       " (12.850469475230561, 45.0),\n",
       " (12.850469475240375, 46.0),\n",
       " (12.85046947524395, 47.0),\n",
       " (12.85046947522877, 48.0),\n",
       " (12.850469475230561, 49.0),\n",
       " (12.850469475242162, 50.0),\n",
       " (50.51439438630578, 51.0),\n",
       " (50.56171446434248, 52.0),\n",
       " (50.51439438631872, 53.0),\n",
       " (50.41962099716843, 54.0),\n",
       " (50.41962099717059, 55.0),\n",
       " (50.51439438631872, 56.0),\n",
       " (50.56171446434248, 57.0),\n",
       " (50.51439438630578, 58.0),\n",
       " (50.56171446434248, 59.0),\n",
       " (50.56171446434033, 60.0),\n",
       " (22.147933257394644, 61.0),\n",
       " (22.43602563139897, 62.0),\n",
       " (22.436025631385856, 63.0),\n",
       " (22.4360256313999, 64.0),\n",
       " (22.1479332573937, 65.0),\n",
       " (22.436025631385856, 66.0),\n",
       " (22.436025631385856, 67.0),\n",
       " (22.436025631400828, 68.0),\n",
       " (22.147933257392754, 69.0),\n",
       " (22.436025631385856, 70.0),\n",
       " (11.564704124815332, 71.0),\n",
       " (11.722841487875636, 72.0),\n",
       " (11.436816325528085, 73.0),\n",
       " (11.722841487875636, 74.0),\n",
       " (11.436816325535226, 75.0),\n",
       " (11.722841487878512, 76.0),\n",
       " (11.564704124818244, 77.0),\n",
       " (11.596697750618631, 78.0),\n",
       " (11.564704124815332, 79.0),\n",
       " (11.596697750618631, 80.0),\n",
       " (11.564704124822395, 81.0),\n",
       " (11.722841487875636, 82.0),\n",
       " (11.436816325528085, 83.0),\n",
       " (11.722841487875636, 84.0),\n",
       " (11.436816325528085, 85.0),\n",
       " (11.722841487881384, 86.0),\n",
       " (11.564704124822395, 87.0),\n",
       " (11.596697750618631, 88.0),\n",
       " (11.564704124815332, 89.0),\n",
       " (11.596697750618631, 90.0),\n",
       " (18.354043731199216, 91.0),\n",
       " (18.354043731185968, 92.0),\n",
       " (18.17330821474484, 93.0),\n",
       " (18.499667088826754, 94.0),\n",
       " (18.20848896285627, 95.0),\n",
       " (18.465041119775933, 96.0),\n",
       " (18.20848896284303, 97.0),\n",
       " (18.499667088840003, 98.0),\n",
       " (18.17330821473157, 99.0),\n",
       " (18.20848896285627, 100.0),\n",
       " (45.29863701043879, 101.0),\n",
       " (45.29863701043879, 102.0),\n",
       " (45.44488375871215, 103.0),\n",
       " (45.29863701043844, 104.0),\n",
       " (45.26615624473773, 105.0),\n",
       " (45.26615624473107, 106.0),\n",
       " (45.29863701043844, 107.0),\n",
       " (45.298637010442114, 108.0),\n",
       " (45.44488375871215, 109.0),\n",
       " (45.298637010435456, 110.0),\n",
       " (0.0, 111.0),\n",
       " (0.0, 112.0),\n",
       " (0.0, 113.0),\n",
       " (0.0, 114.0),\n",
       " (0.0, 115.0),\n",
       " (0.0, 116.0),\n",
       " (0.0, 117.0),\n",
       " (0.0, 118.0),\n",
       " (0.0, 119.0),\n",
       " (0.0, 120.0),\n",
       " (0.0, 121.0),\n",
       " (0.0, 122.0),\n",
       " (0.0, 123.0),\n",
       " (0.0, 124.0),\n",
       " (0.0, 125.0),\n",
       " (0.0, 126.0),\n",
       " (0.0, 127.0),\n",
       " (0.0, 128.0),\n",
       " (0.0, 129.0),\n",
       " (0.0, 130.0)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO : Must check for 'extended' periods of discountinuous frames for future potentially sparese labeled data\n",
    "#TODO : Assuming data is ordered by frame (increasing timesteps/frames)\n",
    "#TODO : Warning : if there are not enough K historic frames then we pre pad with negative ones\n",
    "\n",
    "baboons_to_velocities = {} #maps the baboon's id to it's velocties and respective frames in a tuple\n",
    "\n",
    "for idx, row in training_data.iterrows():\n",
    "    current_frame = row.loc['frame']\n",
    "    current_position = (row.loc['centroid_x'], row.loc['centroid_y'])\n",
    "    current_velocity = row.loc['velocity']\n",
    "    current_id = row.loc['baboon id']\n",
    "    \n",
    "    if current_id not in baboons_to_velocities:\n",
    "        baboons_to_velocities[current_id] = []\n",
    "        \n",
    "    baboons_to_velocities[current_id].append((current_velocity, current_frame))\n",
    "\n",
    "\"\"\"\n",
    "Given a frame returns the k past velocities of the given baboon_id\n",
    "\n",
    "Pre pads with negative ones if there are less than k previous frames available in the labeled data\n",
    "\"\"\"\n",
    "def get_k_past_velocities(current_frame, baboon_id):\n",
    "    velocities = baboons_to_velocities[baboon_id]\n",
    "    \n",
    "    #annoying but I don't know how to do this easier\n",
    "    frame_index = -1\n",
    "    for idx, frame in enumerate(velocities):\n",
    "        if frame[1] == current_frame:\n",
    "            frame_index = idx\n",
    "            break\n",
    "            \n",
    "    if frame_index == -1:\n",
    "        raise RuntimeError('Frame does not exist in dataset for this baboon_id')\n",
    "        \n",
    "    if frame_index < config['k_historic_velocities']:\n",
    "        padded_output = list(np.ones(config['k_historic_velocities'] - frame_index) * -1)   \n",
    "        padded_output.extend([item[0] for item in velocities[:frame_index]])\n",
    "        return padded_output\n",
    "    \n",
    "    else:\n",
    "        return [item[0] for item in velocities[frame_index-config['k_historic_velocities'] : frame_index]]\n",
    "    \n",
    "baboons_to_velocities[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1.0, -1.0, -1.0, 12.85046947525376]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_k_past_velocities(22, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Data and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  3.76458762,  14.74763297,  35.8249973 ,  83.11665914,\n",
       "       407.57334622])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO : way too hard coded here. Make better\n",
    "\n",
    "training_data = pd.read_csv(config['input_training_data'])\n",
    "\n",
    "kmeans = pickle.load(open(config['kmeans_model_path'], 'rb'))\n",
    "kmeans_centers_sorted = np.sort(np.array(kmeans.cluster_centers_).squeeze())\n",
    "config['output_dimension'] = kmeans_centers_sorted.size + 1\n",
    "\n",
    "# test_predict = np.array([10])\n",
    "# print(kmeans.predict(test_predict.reshape(-1,1)))\n",
    "#TODO : add the sitting still !!!\n",
    "label_table = {}\n",
    "label_table[0] = [1,0,0,0,0,0] #sitting\n",
    "label_table[kmeans_centers_sorted[0]] = [0,1,0,0,0,0]\n",
    "label_table[kmeans_centers_sorted[1]] = [0,0,1,0,0,0]\n",
    "label_table[kmeans_centers_sorted[2]] = [0,0,0,1,0,0]\n",
    "label_table[kmeans_centers_sorted[3]] = [0,0,0,0,1,0]\n",
    "label_table[kmeans_centers_sorted[4]] = [0,0,0,0,0,1]\n",
    "\n",
    "\n",
    "kmeans_centers_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12.85046948, 53.82624892, 27.9806787 ,  1.47585282,  1.52817768,\n",
       "       -1.        , -1.        , -1.        , -1.        ])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# [ target v, k nearest baboon's v, k' past velocities ]\n",
    "# X = np.zeros((training_data.shape[0], config['input_dimension']))\n",
    "# labels = np.zeros(())\n",
    "X = []\n",
    "labels = []\n",
    "\n",
    "start_historic = 1 + config['k_nearest_baboons']\n",
    "\n",
    "\n",
    "for idx, row in training_data.iterrows():\n",
    "    current_frame = row.loc['frame']\n",
    "    current_position = (row.loc['centroid_x'], row.loc['centroid_y'])\n",
    "    current_velocity = row.loc['velocity']\n",
    "    current_id = row.loc['baboon id']\n",
    "    \n",
    "    current_label = training_data.loc[(training_data['frame'] == (current_frame + 1)) & (training_data['baboon id'] == current_id)]\n",
    "    if(current_label.empty):\n",
    "        #last frame for baboon, therefore no label\n",
    "        continue\n",
    "        \n",
    "    #create the label\n",
    "    current_one_hot = []\n",
    "    current_label_velocity = current_label['velocity'].to_numpy()\n",
    "    if current_label_velocity[0] == 0:\n",
    "        current_one_hot = label_table[0]\n",
    "    else:\n",
    "        kmeans_label = kmeans.predict(current_label_velocity.reshape(-1,1))[0]\n",
    "        current_one_hot = label_table[ kmeans.cluster_centers_[kmeans_label][0] ]\n",
    "    current_one_hot = np.array(current_one_hot)\n",
    "    \n",
    "    #create the datapoint\n",
    "    current_datapoint = np.zeros(config['input_dimension'])\n",
    "    current_datapoint[0] = current_velocity\n",
    "    current_datapoint[1:start_historic] = get_k_nearest_baboons_velocities(current_frame, current_id)\n",
    "    current_datapoint[start_historic:] = get_k_past_velocities(current_frame, current_id)\n",
    "    \n",
    "    X.append(current_datapoint)\n",
    "    labels.append(current_one_hot)\n",
    "\n",
    "\n",
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((52990, 9), (52990, 6))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array(X)\n",
    "labels = np.array(labels)\n",
    "\n",
    "X.shape, labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA NOT supported\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Nnet(\n",
       "  (input_layer): Sequential(\n",
       "    (0): Linear(in_features=9, out_features=500, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (main): Sequential(\n",
       "    (0): Linear(in_features=500, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=25088, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Dropout(p=0.5, inplace=False)\n",
       "    (9): Linear(in_features=4096, out_features=500, bias=True)\n",
       "    (10): ReLU(inplace=True)\n",
       "    (11): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (output): Linear(in_features=500, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO : add weighted classes to save from an unbalanced dataset\n",
    "# Check if your system supports CUDA\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "# Setup GPU optimization if CUDA is supported\n",
    "if use_cuda:\n",
    "    computing_device = torch.device(\"cuda\")\n",
    "    extras = {\"num_workers\": 1, \"pin_memory\": True}\n",
    "    print(\"CUDA is supported\")\n",
    "else: # Otherwise, train on the CPU\n",
    "    computing_device = torch.device(\"cpu\")\n",
    "    extras = False\n",
    "    print(\"CUDA NOT supported\")\n",
    "    \n",
    " \n",
    "if not config['checkpoint_training']:\n",
    "    net = Nnet(config['input_dimension'], config['output_dimension']).to(computing_device)\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(),lr = config['learning_rate'])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = loader(X, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = config['batch_size']\n",
    "validation_split = config['val_split']\n",
    "shuffle_dataset = config['shuffle_dataset']\n",
    "random_seed= 69 \n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, \n",
    "                                               sampler=train_sampler)\n",
    "    \n",
    "validation_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                                sampler=valid_sampler)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN!1!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started training epoch : 0\n",
      "Minibatch 0 loss : 0.5800476670265198\n",
      "Minibatch 1 loss : 0.4719441533088684\n",
      "Minibatch 2 loss : 0.4851217269897461\n",
      "Minibatch 3 loss : 1.1768085956573486\n",
      "Minibatch 4 loss : 0.5792779922485352\n",
      "Minibatch 5 loss : 0.3689684271812439\n",
      "Minibatch 6 loss : 0.7100775241851807\n",
      "Minibatch 7 loss : 0.6991098523139954\n",
      "Minibatch 8 loss : 1.085209608078003\n",
      "Minibatch 9 loss : 0.3221856951713562\n",
      "Minibatch 10 loss : 0.7733945250511169\n",
      "Minibatch 11 loss : 0.5279470682144165\n",
      "Minibatch 12 loss : 0.307323157787323\n",
      "Minibatch 13 loss : 0.49710744619369507\n",
      "Minibatch 14 loss : 0.7257239818572998\n",
      "Minibatch 15 loss : 0.4586210250854492\n",
      "Minibatch 16 loss : 0.47458377480506897\n",
      "Minibatch 17 loss : 0.6399723887443542\n",
      "Minibatch 18 loss : 0.310200035572052\n",
      "Minibatch 19 loss : 0.7783452272415161\n",
      "Minibatch 20 loss : 0.47781839966773987\n",
      "Minibatch 21 loss : 0.7255041599273682\n",
      "Minibatch 22 loss : 0.3601127564907074\n",
      "Minibatch 23 loss : 0.5080505609512329\n",
      "Minibatch 24 loss : 1.7527594566345215\n",
      "Minibatch 25 loss : 0.381939560174942\n",
      "Minibatch 26 loss : 0.478908896446228\n",
      "Minibatch 27 loss : 0.6771794557571411\n",
      "Minibatch 28 loss : 0.4183613061904907\n",
      "Minibatch 29 loss : 0.7082062363624573\n",
      "Minibatch 30 loss : 0.5787190198898315\n",
      "Minibatch 31 loss : 0.4334961175918579\n",
      "Minibatch 32 loss : 1.244372844696045\n",
      "Minibatch 33 loss : 0.6468930244445801\n",
      "Minibatch 34 loss : 0.5892461538314819\n",
      "Minibatch 35 loss : 0.45293352007865906\n",
      "Minibatch 36 loss : 0.7585873007774353\n",
      "Minibatch 37 loss : 0.45193251967430115\n",
      "Minibatch 38 loss : 0.5208256840705872\n",
      "Minibatch 39 loss : 0.7477090358734131\n",
      "Minibatch 40 loss : 0.6652827858924866\n",
      "Minibatch 41 loss : 0.8953191041946411\n",
      "Minibatch 42 loss : 0.6677836775779724\n",
      "Minibatch 43 loss : 0.5054005980491638\n",
      "Minibatch 44 loss : 0.6530317664146423\n",
      "Minibatch 45 loss : 0.5941755175590515\n",
      "Minibatch 46 loss : 0.6357078552246094\n",
      "Minibatch 47 loss : 0.5273440480232239\n",
      "Minibatch 48 loss : 0.516889214515686\n",
      "Minibatch 49 loss : 0.5207136869430542\n",
      "Epoch 1, average minibatch 50 loss: 0.6213434934616089\n",
      "Minibatch 50 loss : 0.8701925277709961\n",
      "Minibatch 51 loss : 0.569002628326416\n",
      "Minibatch 52 loss : 0.4458993673324585\n",
      "Minibatch 53 loss : 0.24857009947299957\n",
      "Minibatch 54 loss : 0.8499717116355896\n",
      "Minibatch 55 loss : 0.6215348839759827\n",
      "Minibatch 56 loss : 0.45196476578712463\n",
      "Minibatch 57 loss : 0.6292175054550171\n",
      "Minibatch 58 loss : 1.2331730127334595\n",
      "Minibatch 59 loss : 1.798945665359497\n",
      "Minibatch 60 loss : 0.41266024112701416\n",
      "Minibatch 61 loss : 0.5474255681037903\n",
      "Minibatch 62 loss : 0.5661909580230713\n",
      "Minibatch 63 loss : 0.7460615038871765\n",
      "Minibatch 64 loss : 0.47702866792678833\n",
      "Minibatch 65 loss : 0.5110100507736206\n",
      "Minibatch 66 loss : 0.6843992471694946\n",
      "Minibatch 67 loss : 1.721930980682373\n",
      "Minibatch 68 loss : 0.3514229357242584\n",
      "Minibatch 69 loss : 0.6206157803535461\n",
      "Minibatch 70 loss : 0.5060888528823853\n",
      "Minibatch 71 loss : 0.5611381530761719\n",
      "Minibatch 72 loss : 0.7113872766494751\n",
      "Minibatch 73 loss : 0.28856027126312256\n",
      "Minibatch 74 loss : 0.9188835024833679\n",
      "Minibatch 75 loss : 0.31197288632392883\n",
      "Minibatch 76 loss : 0.3516586422920227\n",
      "Minibatch 77 loss : 0.35038965940475464\n",
      "Minibatch 78 loss : 0.3957807123661041\n",
      "Minibatch 79 loss : 0.5156330466270447\n",
      "Minibatch 80 loss : 0.9424691200256348\n",
      "Minibatch 81 loss : 0.7511615753173828\n",
      "Minibatch 82 loss : 0.20502804219722748\n",
      "Minibatch 83 loss : 0.4337388277053833\n",
      "Minibatch 84 loss : 0.944776713848114\n",
      "Minibatch 85 loss : 0.9623277187347412\n",
      "Minibatch 86 loss : 0.39306169748306274\n",
      "Minibatch 87 loss : 0.5850422382354736\n",
      "Minibatch 88 loss : 0.4218837022781372\n",
      "Minibatch 89 loss : 0.5335681438446045\n",
      "Minibatch 90 loss : 0.3493102192878723\n",
      "Minibatch 91 loss : 0.4671369791030884\n",
      "Minibatch 92 loss : 0.5257721543312073\n",
      "Minibatch 93 loss : 0.3862352967262268\n",
      "Minibatch 94 loss : 0.4183775782585144\n",
      "Minibatch 95 loss : 0.7102846503257751\n",
      "Minibatch 96 loss : 0.29482749104499817\n",
      "Minibatch 97 loss : 0.2760978937149048\n",
      "Minibatch 98 loss : 0.4725935757160187\n",
      "Minibatch 99 loss : 0.7932558655738831\n",
      "Epoch 1, average minibatch 100 loss: 0.6027133464813232\n",
      "Minibatch 100 loss : 0.7265171408653259\n",
      "Minibatch 101 loss : 0.7989664077758789\n",
      "Minibatch 102 loss : 0.36957743763923645\n",
      "Minibatch 103 loss : 2.0180935859680176\n",
      "Minibatch 104 loss : 0.5103796720504761\n",
      "Minibatch 105 loss : 0.6682344079017639\n",
      "Minibatch 106 loss : 1.522121787071228\n",
      "Minibatch 107 loss : 0.7763683199882507\n",
      "Minibatch 108 loss : 0.3493959307670593\n",
      "Minibatch 109 loss : 0.652733325958252\n",
      "Minibatch 110 loss : 0.5175081491470337\n",
      "Minibatch 111 loss : 0.40260547399520874\n",
      "Minibatch 112 loss : 0.42799198627471924\n",
      "Minibatch 113 loss : 0.2896486520767212\n",
      "Minibatch 114 loss : 0.4865637719631195\n",
      "Minibatch 115 loss : 0.3926396071910858\n",
      "Minibatch 116 loss : 0.6705693006515503\n",
      "Minibatch 117 loss : 0.6944611668586731\n",
      "Minibatch 118 loss : 0.43419981002807617\n",
      "Minibatch 119 loss : 0.6544840335845947\n",
      "Minibatch 120 loss : 0.3962252736091614\n",
      "Minibatch 121 loss : 0.7098523378372192\n",
      "Minibatch 122 loss : 0.33408695459365845\n",
      "Minibatch 123 loss : 0.2855522930622101\n",
      "Minibatch 124 loss : 1.3397419452667236\n",
      "Minibatch 125 loss : 0.6969556212425232\n",
      "Minibatch 126 loss : 0.1998768150806427\n",
      "Minibatch 127 loss : 1.330950140953064\n",
      "Minibatch 128 loss : 0.293362557888031\n",
      "Minibatch 129 loss : 0.30768871307373047\n",
      "Minibatch 130 loss : 0.439558744430542\n",
      "Minibatch 131 loss : 1.4145039319992065\n",
      "Minibatch 132 loss : 0.8234331011772156\n",
      "Minibatch 133 loss : 0.5133316516876221\n",
      "Minibatch 134 loss : 0.6447939872741699\n",
      "Minibatch 135 loss : 0.8014802932739258\n",
      "Minibatch 136 loss : 0.6895433664321899\n",
      "Minibatch 137 loss : 0.7692729830741882\n",
      "Minibatch 138 loss : 0.4777120053768158\n",
      "Minibatch 139 loss : 0.469108521938324\n",
      "Minibatch 140 loss : 0.5388141870498657\n",
      "Minibatch 141 loss : 0.49713611602783203\n",
      "Minibatch 142 loss : 1.0191396474838257\n",
      "Minibatch 143 loss : 0.6112807393074036\n",
      "Minibatch 144 loss : 0.37849152088165283\n",
      "Minibatch 145 loss : 0.39502835273742676\n",
      "Minibatch 146 loss : 0.5402716994285583\n",
      "Minibatch 147 loss : 0.738233745098114\n",
      "Minibatch 148 loss : 0.5880213379859924\n",
      "Minibatch 149 loss : 0.4379757344722748\n",
      "Epoch 1, average minibatch 150 loss: 0.6408896446228027\n",
      "Minibatch 150 loss : 0.4327104091644287\n",
      "Minibatch 151 loss : 0.3937186002731323\n",
      "Minibatch 152 loss : 1.0150481462478638\n",
      "Minibatch 153 loss : 0.7035223245620728\n",
      "Minibatch 154 loss : 0.34095048904418945\n",
      "Minibatch 155 loss : 1.1934562921524048\n",
      "Minibatch 156 loss : 0.6501834988594055\n",
      "Minibatch 157 loss : 0.3392237722873688\n",
      "Minibatch 158 loss : 0.35531193017959595\n",
      "Minibatch 159 loss : 0.4569218158721924\n",
      "Minibatch 160 loss : 0.45090365409851074\n",
      "Minibatch 161 loss : 0.5339369773864746\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-40ec79565a73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# Update the weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;31m# Add this iteration's loss to the total_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mtotal_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    109\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Track the loss across training\n",
    "total_loss = []\n",
    "avg_minibatch_loss = []\n",
    "num_times_incraesed = 0\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "N = 50\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#TODO: Add dropout! When we do we MUST use model.eval() for val or testing and model.train() for training\n",
    "prev_val_loss = float(\"inf\")\n",
    "\n",
    "for epoch in range(config['epochs']):    \n",
    "    old_net_weights = net.state_dict().copy()\n",
    "    old_optimizer = optimizer.state_dict().copy()\n",
    "    \n",
    "    print(f\"Started training epoch : {epoch}\" )\n",
    "    \n",
    "\n",
    "    N_minibatch_loss = 0.0\n",
    "    total_epoch_loss = 0.0\n",
    "    average_epoch_loss = 0.0\n",
    "    num_minibatches = 0\n",
    "    \n",
    "     \n",
    "    net.train() #turns dropout on\n",
    "\n",
    "    # Get the next minibatch of images, labels for training\n",
    "    for minibatch_count, (datapoints, labels) in enumerate(train_loader, 0):\n",
    "\n",
    "        num_minibatches += 1\n",
    "        # Zero out the stored gradient (buffer) from the previous iteration\n",
    "        optimizer.zero_grad()\n",
    "        # Put the minibatch data in CUDA Tensors and run on the GPU if supported\n",
    "        datapoints, labels = datapoints.to(computing_device), labels.to(computing_device)\n",
    "        # Perform the forward pass through the network and compute the loss\n",
    "        outputs = net(datapoints.float())\n",
    "        labels = torch.max(labels, 1)[1]\n",
    "        \n",
    "        #computing the CEL using the net and the labels\n",
    "        loss = criterion(outputs, labels)\n",
    "#         print(f'Minibatch {minibatch_count} loss : {loss.item()}')\n",
    "        \n",
    "        # Automagically compute the gradients and backpropagate the loss through the network\n",
    "        loss.backward()\n",
    "        total_epoch_loss += loss.item()\n",
    "\n",
    "        # Update the weights\n",
    "        optimizer.step()    \n",
    "        # Add this iteration's loss to the total_loss\n",
    "        total_loss.append(loss.item())\n",
    "        N_minibatch_loss += loss\n",
    "        \n",
    "               \n",
    "        \n",
    "        if minibatch_count % N == 49:\n",
    "            #Print the loss averaged over the last N mini-batches\n",
    "            N_minibatch_loss /= N\n",
    "            print(f'Epoch {epoch + 1}, average minibatch {minibatch_count+1} loss: {N_minibatch_loss}')\n",
    "            # Add the averaged loss over N minibatches and reset the counter\n",
    "            avg_minibatch_loss.append(N_minibatch_loss)\n",
    "            N_minibatch_loss = 0.0\n",
    "    \n",
    "    average_epoch_loss = total_epoch_loss / num_minibatches\n",
    "    print(f\"Finished {epoch+ 1} epochs of training with average {average_epoch_loss} loss.\" )\n",
    "    \n",
    "    N_minibatch_val_loss = 0\n",
    "    val_data_size = 0\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        net.eval() #turns dropout off\n",
    "        print(\"validation starting: \")\n",
    "\n",
    "        for minibatch_count, (datapoints, labels) in enumerate(validation_loader, 0):\n",
    "            val_data_size += 1\n",
    "            datapoints, labels = images.to(computing_device), labels.to(computing_device)\n",
    "            outputs = net(datapoints)\n",
    "            val_loss = criterion(outputs, labels).item()\n",
    "            N_minibatch_val_loss += val_loss\n",
    "\n",
    "        N_minibatch_val_loss /= val_data_size\n",
    "        print(f'Epoch {epoch + 1} average validation loss over {val_data_size} datapoints : {N_minibatch_val_loss}' )\n",
    "        \n",
    "        #early stopping\n",
    "        if config['early_stop']:\n",
    "            print(str(N_minibatch_val_loss) + ' vs' + str(prev_val_loss))\n",
    "\n",
    "            if num_times_incraesed >= config['early_stop_epoch']:\n",
    "                print('early stopping triggered')\n",
    "                break\n",
    "            if N_minibatch_val_loss > prev_val_loss:\n",
    "                print('keeping old weights')\n",
    "                num_times_incraesed += 1\n",
    "                net.load_state_dict(old_net_weights)\n",
    "                optimizer.load_state_dict(old_optimizer)\n",
    "            else : \n",
    "                print('val is less than previous')\n",
    "                num_times_incraesed = 0\n",
    "                prev_val_loss = N_minibatch_val_loss\n",
    "\n",
    "             \n",
    "    training_losses.append(average_epoch_loss)\n",
    "    validation_losses.append(N_minibatch_val_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([], [])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_losses, validation_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'epochs_plots' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-1e2611c72932>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs_plots\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_plots\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs_plots\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_plots\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'upper right'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'epochs_plots' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "min_entries = min(len(training_losses), len(validation_losses))\n",
    "for i in range(min_entries):\n",
    "    epochs_plots.append(i+1)\n",
    "    \n",
    "val_plots = training_losses\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs_plots, val_plots, label='val loss')\n",
    "plt.plot(epochs_plots, train_plots, label='train loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('cross entropy error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
