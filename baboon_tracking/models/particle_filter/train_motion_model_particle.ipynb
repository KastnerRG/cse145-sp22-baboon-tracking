{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please read the TODOs as they may be crucial depending on the dataset you are using to train. Configs are not specific to a dataset so choose those as you so appropriate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from baseline_model import *\n",
    "from pathlib import Path \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Path('../../..')\n",
    "\n",
    "config = {}\n",
    "config['k_nearest_baboons'] = 4 #getting the four nearest baboons per frame to consider(MUST BE EVEN)\n",
    "config['k_historic_velocities'] = 4 #getting the four past velocities to consider\n",
    "config['checkpoint_training'] = False\n",
    "config['saved_path'] = 'saved_weights.pth' #path to save current model information (epoch, weights, etc...)\n",
    "config['learning_rate'] = 0.001 #lr\n",
    "config['batch_size'] = 32 # Number of training samples per batch to be passed to network\n",
    "config['val_split'] = 0.2 #ratio of val and training data\n",
    "config['shuffle_dataset'] = True #shuffle data before start (TODO: change this if we use kfold?)\n",
    "config['epochs'] = 100  # Number of epochs to train the model\n",
    "config['early_stop'] = True  # Implement early stopping or not\n",
    "config['early_stop_epoch'] = 3 # Number of epochs for which validation loss increases to be counted as overfitting\n",
    "config['input_training_data'] = root / 'output/DJI_0870_velocity.csv'\n",
    "config['kmeans_model_path'] = root / 'output' / 'velocity_model.pkl'\n",
    "\n",
    "\n",
    "#DO NOT CHANGE THE FOLLOWING \n",
    "config['input_dimension'] = config['k_nearest_baboons'] + config['k_historic_velocities'] + 1 #DON'T TOUCH THIS\n",
    "config['validation_loss_path'] = '' #leave empty \n",
    "config['training_loss_path'] = '' #leave empty \n",
    "config['output_dimension'] = -1 #leave empty\n",
    "\n",
    "#config checks\n",
    "assert( config['k_nearest_baboons'] % 2 == 0 )\n",
    "assert( os.path.isfile(config['input_training_data']) == True)\n",
    "assert( os.path.isfile(config['kmeans_model_path']) == True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize table for getting nearest baboons and their respective velocities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((2125.4, 1057.18), 51.074506175125435, 10.0),\n",
       " ((2053.7, 939.645), 0.44955044955004064, 11.0),\n",
       " ((2045.13, 905.955), 13.382885251805144, 12.0),\n",
       " ((2007.215, 900.905), 0.6178479958454139, 13.0),\n",
       " ((2161.0550000000007, 941.54), 3.7101154055304586, 14.0),\n",
       " ((2278.565, 1158.07), 6.985331880593899, 15.0),\n",
       " ((2314.695, 1177.34), 12.901044905119257, 16.0),\n",
       " ((2119.07, 809.89), 19.39097995929645, 17.0),\n",
       " ((1908.015, 763.5), 1.8776169959325903, 18.0),\n",
       " ((1923.885, 781.475), 6.434838327744581, 19.0),\n",
       " ((1950.245, 810.35), 1.6483516483603706, 20.0),\n",
       " ((1948.19, 826.1949999999998), 2.1455276318607357, 21.0),\n",
       " ((2105.445, 1237.58), 1.2081305317123932, 22.0),\n",
       " ((2124.105, 1239.65), 0.9477355624850088, 23.0),\n",
       " ((2490.31, 1190.72), 3.146853146853692, 24.0),\n",
       " ((2487.08, 1277.885), 1.0805847978347027, 25.0),\n",
       " ((1707.42, 1122.74), 0.14985014984660636, 26.0),\n",
       " ((1785.4699999999998, 1085.1), 15.050804630763162, 27.0),\n",
       " ((1800.63, 1090.315), 3.8526354042502913, 28.0),\n",
       " ((1865.715, 1096.415), 2.5781194606482063, 29.0),\n",
       " ((1936.615, 1130.0749999999996), 2.4393837057189582, 30.0),\n",
       " ((1836.705, 1101.535), 0.9477355624957832, 31.0),\n",
       " ((1514.1950000000004, 1070.125), 14.424585076251118, 32.0),\n",
       " ((1708.935, 1301.365), 1.0596005712115035, 33.0),\n",
       " ((2106.57, 1522.23), 21.45998549660368, 34.0),\n",
       " ((2046.85, 1537.89), 5.310702841163275, 35.0),\n",
       " ((2721.89, 1494.69), 0.6701502430032729, 36.0),\n",
       " ((2202.05, 1108.05), 0.2997002997068415, 37.0),\n",
       " ((2227.535, 1120.34), 0.9115028766610744, 38.0),\n",
       " ((2031.965, 816.7249999999998), 1.5281776764053896, 39.0),\n",
       " ((2057.76, 848.5), 1.5644814948393388, 40.0),\n",
       " ((2080.02, 859.9449999999998), 1.613935506635413, 41.0)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This is assuming the obvious assumption that a baboon only has one position per frame\n",
    "#TODO : Assuming data is ordered by frame (increasing timesteps/frames)\n",
    "training_data = pd.read_csv(config['input_training_data'])\n",
    "\n",
    "frames_to_velocities = {} #Maps a single frame to a list of all the baboons, their id, position, and velocity\n",
    "\n",
    "for idx, row in training_data.iterrows():\n",
    "    current_frame = row.loc['frame']\n",
    "    current_position = (row.loc['centroid_x'], row.loc['centroid_y'])\n",
    "    current_velocity = row.loc['velocity']\n",
    "    current_id = row.loc['baboon id']\n",
    "    \n",
    "    if current_frame not in frames_to_velocities:\n",
    "        frames_to_velocities[current_frame] = []\n",
    "    \n",
    "    frames_to_velocities[current_frame].append((current_position, current_velocity, current_id))\n",
    "\n",
    "\"\"\"\n",
    "Returns the K physically nearest baboons to baboon_id at the given frame\n",
    "\n",
    "Pre-pends with negative ones if there are less than k nearest baboons in the frame\n",
    "\"\"\"\n",
    "def get_k_nearest_baboons_velocities(frame, baboon_id):\n",
    "    baboons_in_frame = frames_to_velocities[frame].copy()\n",
    "    output = []\n",
    "    \n",
    "    target_position = (-1, -1)\n",
    "    \n",
    "    #get rid of the baboon we are considering\n",
    "    for idx, baboon in enumerate(baboons_in_frame):\n",
    "        if baboon[2] == baboon_id:\n",
    "            target_velocity = baboons_in_frame[idx][0]\n",
    "            del baboons_in_frame[idx]\n",
    "            break\n",
    "      \n",
    "    if len(baboons_in_frame) <= config['k_nearest_baboons']:\n",
    "        padded_output = list(np.ones(config['k_nearest_baboons'] - len(baboons_in_frame)) * -1)\n",
    "        return padded_output.extend([item[1] for item in baboons_in_frame])\n",
    "    \n",
    "    for k_nearest_neighbor in range(config['k_nearest_baboons']):\n",
    "        min_distance = math.inf\n",
    "        min_velocity = -1\n",
    "        min_idx = -1\n",
    "        \n",
    "        for idx, baboon in enumerate(baboons_in_frame):\n",
    "            current_distance = np.linalg.norm(np.array(baboon[0]) - np.array(target_velocity))\n",
    "            if current_distance < min_distance:\n",
    "                min_distance = current_distance\n",
    "                min_velocity = baboon[1]\n",
    "                min_idx = idx\n",
    "        \n",
    "        output.append(min_velocity)\n",
    "        del baboons_in_frame[min_idx]\n",
    "        \n",
    "    return output\n",
    "            \n",
    "frames_to_velocities[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.2997002997068415,\n",
       " 0.9115028766610744,\n",
       " 3.7101154055304586,\n",
       " 0.44955044955004064]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_k_nearest_baboons_velocities(1, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To get the historic velocities per baboon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(12.85046947525376, 21.0),\n",
       " (12.97826878451604, 22.0),\n",
       " (12.644369496333105, 23.0),\n",
       " (12.850469475230561, 24.0),\n",
       " (12.85046947525376, 25.0),\n",
       " (12.85046947522877, 26.0),\n",
       " (12.850469475230561, 27.0),\n",
       " (12.850469475242162, 28.0),\n",
       " (12.978268784517812, 29.0),\n",
       " (12.723151376903115, 30.0),\n",
       " (12.85046947522877, 31.0),\n",
       " (12.978268784519587, 32.0),\n",
       " (12.850469475240375, 33.0),\n",
       " (12.723151376903115, 34.0),\n",
       " (12.978268784517812, 35.0),\n",
       " (12.85046947522877, 36.0),\n",
       " (12.723151376891556, 37.0),\n",
       " (12.85046947525376, 38.0),\n",
       " (12.97826878451604, 39.0),\n",
       " (12.850469475232348, 40.0),\n",
       " (12.723151376903115, 41.0),\n",
       " (12.978268784517812, 42.0),\n",
       " (12.850469475240375, 43.0),\n",
       " (12.850469475230561, 44.0),\n",
       " (12.850469475230561, 45.0),\n",
       " (12.850469475240375, 46.0),\n",
       " (12.85046947524395, 47.0),\n",
       " (12.85046947522877, 48.0),\n",
       " (12.850469475230561, 49.0),\n",
       " (12.850469475242162, 50.0),\n",
       " (50.51439438630578, 51.0),\n",
       " (50.56171446434248, 52.0),\n",
       " (50.51439438631872, 53.0),\n",
       " (50.41962099716843, 54.0),\n",
       " (50.41962099717059, 55.0),\n",
       " (50.51439438631872, 56.0),\n",
       " (50.56171446434248, 57.0),\n",
       " (50.51439438630578, 58.0),\n",
       " (50.56171446434248, 59.0),\n",
       " (50.56171446434033, 60.0),\n",
       " (22.147933257394644, 61.0),\n",
       " (22.43602563139897, 62.0),\n",
       " (22.436025631385856, 63.0),\n",
       " (22.4360256313999, 64.0),\n",
       " (22.1479332573937, 65.0),\n",
       " (22.436025631385856, 66.0),\n",
       " (22.436025631385856, 67.0),\n",
       " (22.436025631400828, 68.0),\n",
       " (22.147933257392754, 69.0),\n",
       " (22.436025631385856, 70.0),\n",
       " (11.564704124815332, 71.0),\n",
       " (11.722841487875636, 72.0),\n",
       " (11.436816325528085, 73.0),\n",
       " (11.722841487875636, 74.0),\n",
       " (11.436816325535226, 75.0),\n",
       " (11.722841487878512, 76.0),\n",
       " (11.564704124818244, 77.0),\n",
       " (11.596697750618631, 78.0),\n",
       " (11.564704124815332, 79.0),\n",
       " (11.596697750618631, 80.0),\n",
       " (11.564704124822395, 81.0),\n",
       " (11.722841487875636, 82.0),\n",
       " (11.436816325528085, 83.0),\n",
       " (11.722841487875636, 84.0),\n",
       " (11.436816325528085, 85.0),\n",
       " (11.722841487881384, 86.0),\n",
       " (11.564704124822395, 87.0),\n",
       " (11.596697750618631, 88.0),\n",
       " (11.564704124815332, 89.0),\n",
       " (11.596697750618631, 90.0),\n",
       " (18.354043731199216, 91.0),\n",
       " (18.354043731185968, 92.0),\n",
       " (18.17330821474484, 93.0),\n",
       " (18.499667088826754, 94.0),\n",
       " (18.20848896285627, 95.0),\n",
       " (18.465041119775933, 96.0),\n",
       " (18.20848896284303, 97.0),\n",
       " (18.499667088840003, 98.0),\n",
       " (18.17330821473157, 99.0),\n",
       " (18.20848896285627, 100.0),\n",
       " (45.29863701043879, 101.0),\n",
       " (45.29863701043879, 102.0),\n",
       " (45.44488375871215, 103.0),\n",
       " (45.29863701043844, 104.0),\n",
       " (45.26615624473773, 105.0),\n",
       " (45.26615624473107, 106.0),\n",
       " (45.29863701043844, 107.0),\n",
       " (45.298637010442114, 108.0),\n",
       " (45.44488375871215, 109.0),\n",
       " (45.298637010435456, 110.0),\n",
       " (0.0, 111.0),\n",
       " (0.0, 112.0),\n",
       " (0.0, 113.0),\n",
       " (0.0, 114.0),\n",
       " (0.0, 115.0),\n",
       " (0.0, 116.0),\n",
       " (0.0, 117.0),\n",
       " (0.0, 118.0),\n",
       " (0.0, 119.0),\n",
       " (0.0, 120.0),\n",
       " (0.0, 121.0),\n",
       " (0.0, 122.0),\n",
       " (0.0, 123.0),\n",
       " (0.0, 124.0),\n",
       " (0.0, 125.0),\n",
       " (0.0, 126.0),\n",
       " (0.0, 127.0),\n",
       " (0.0, 128.0),\n",
       " (0.0, 129.0),\n",
       " (0.0, 130.0)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO : Must check for 'extended' periods of discountinuous frames for future potentially sparese labeled data\n",
    "#TODO : Assuming data is ordered by frame (increasing timesteps/frames)\n",
    "#TODO : Warning : if there are not enough K historic frames then we pre pad with negative ones\n",
    "\n",
    "baboons_to_velocities = {} #maps the baboon's id to it's velocties and respective frames in a tuple\n",
    "\n",
    "for idx, row in training_data.iterrows():\n",
    "    current_frame = row.loc['frame']\n",
    "    current_position = (row.loc['centroid_x'], row.loc['centroid_y'])\n",
    "    current_velocity = row.loc['velocity']\n",
    "    current_id = row.loc['baboon id']\n",
    "    \n",
    "    if current_id not in baboons_to_velocities:\n",
    "        baboons_to_velocities[current_id] = []\n",
    "        \n",
    "    baboons_to_velocities[current_id].append((current_velocity, current_frame))\n",
    "\n",
    "\"\"\"\n",
    "Given a frame returns the k past velocities of the given baboon_id\n",
    "\n",
    "Pre pads with negative ones if there are less than k previous frames available in the labeled data\n",
    "\"\"\"\n",
    "def get_k_past_velocities(current_frame, baboon_id):\n",
    "    velocities = baboons_to_velocities[baboon_id]\n",
    "    \n",
    "    #annoying but I don't know how to do this easier\n",
    "    frame_index = -1\n",
    "    for idx, frame in enumerate(velocities):\n",
    "        if frame[1] == current_frame:\n",
    "            frame_index = idx\n",
    "            break\n",
    "            \n",
    "    if frame_index == -1:\n",
    "        raise RuntimeError('Frame does not exist in dataset for this baboon_id')\n",
    "        \n",
    "    if frame_index < config['k_historic_velocities']:\n",
    "        padded_output = list(np.ones(config['k_historic_velocities'] - frame_index) * -1)   \n",
    "        padded_output.extend([item[0] for item in velocities[:frame_index]])\n",
    "        return padded_output\n",
    "    \n",
    "    else:\n",
    "        return [item[0] for item in velocities[frame_index-config['k_historic_velocities'] : frame_index]]\n",
    "    \n",
    "baboons_to_velocities[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1.0, -1.0, -1.0, 12.85046947525376]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_k_past_velocities(22, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Data and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({3.7645876213667497: [1, 0, 0, 0, 0],\n",
       "  14.747632965184385: [0, 1, 0, 0, 0],\n",
       "  35.824997295367055: [0, 0, 1, 0, 0],\n",
       "  83.11665913549479: [0, 0, 0, 1, 0],\n",
       "  407.57334622113547: [0, 0, 0, 0, 1]},\n",
       " array([[ 14.74763297],\n",
       "        [ 35.8249973 ],\n",
       "        [ 83.11665914],\n",
       "        [  3.76458762],\n",
       "        [407.57334622]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO : way too hard coded here. Make better\n",
    "\n",
    "training_data = pd.read_csv(config['input_training_data'])\n",
    "\n",
    "kmeans = pickle.load(open(config['kmeans_model_path'], 'rb'))\n",
    "kmeans_centers_sorted = np.sort(np.array(kmeans.cluster_centers_).squeeze())\n",
    "config['output_dimension'] = kmeans_centers_sorted.size\n",
    "\n",
    "# test_predict = np.array([10])\n",
    "# print(kmeans.predict(test_predict.reshape(-1,1)))\n",
    "label_table = {}\n",
    "label_table[kmeans_centers_sorted[0]] = [1,0,0,0,0]\n",
    "label_table[kmeans_centers_sorted[1]] = [0,1,0,0,0]\n",
    "label_table[kmeans_centers_sorted[2]] = [0,0,1,0,0]\n",
    "label_table[kmeans_centers_sorted[3]] = [0,0,0,1,0]\n",
    "label_table[kmeans_centers_sorted[4]] = [0,0,0,0,1]\n",
    "\n",
    "\n",
    "label_table, kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12.85046948, 53.82624892, 27.9806787 ,  1.47585282,  1.52817768,\n",
       "       -1.        , -1.        , -1.        , -1.        ])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# [ target v, k nearest baboon's v, k' past velocities ]\n",
    "# X = np.zeros((training_data.shape[0], config['input_dimension']))\n",
    "# labels = np.zeros(())\n",
    "X = []\n",
    "labels = []\n",
    "\n",
    "start_historic = 1 + config['k_nearest_baboons']\n",
    "\n",
    "\n",
    "for idx, row in training_data.iterrows():\n",
    "    current_frame = row.loc['frame']\n",
    "    current_position = (row.loc['centroid_x'], row.loc['centroid_y'])\n",
    "    current_velocity = row.loc['velocity']\n",
    "    current_id = row.loc['baboon id']\n",
    "    \n",
    "    current_label = training_data.loc[(training_data['frame'] == (current_frame + 1)) & (training_data['baboon id'] == current_id)]\n",
    "    if(current_label.empty):\n",
    "        #last frame for baboon, therefore no label\n",
    "        continue\n",
    "        \n",
    "    current_label_velocity = current_label['velocity'].to_numpy()\n",
    "    kmeans_label = kmeans.predict(current_label_velocity.reshape(-1,1))[0]\n",
    "    current_label = label_table[ kmeans.cluster_centers_[kmeans_label][0] ]\n",
    "    current_label = np.array(current_label)    \n",
    "    \n",
    "    current_datapoint = np.zeros(config['input_dimension'])\n",
    "    current_datapoint[0] = current_velocity\n",
    "    current_datapoint[1:start_historic] = get_k_nearest_baboons_velocities(current_frame, current_id)\n",
    "    current_datapoint[start_historic:] = get_k_past_velocities(current_frame, current_id)\n",
    "    \n",
    "    X.append(current_datapoint)\n",
    "    labels.append(current_label)\n",
    "\n",
    "\n",
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((52990, 9), (52990, 5))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array(X)\n",
    "labels = np.array(labels)\n",
    "\n",
    "X.shape, labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA NOT supported\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'output_dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-5cecf6503c49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'checkpoint_training'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_dimension'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomputing_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'learning_rate'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'output_dim'"
     ]
    }
   ],
   "source": [
    "# Check if your system supports CUDA\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "# Setup GPU optimization if CUDA is supported\n",
    "if use_cuda:\n",
    "    computing_device = torch.device(\"cuda\")\n",
    "    extras = {\"num_workers\": 1, \"pin_memory\": True}\n",
    "    print(\"CUDA is supported\")\n",
    "else: # Otherwise, train on the CPU\n",
    "    computing_device = torch.device(\"cpu\")\n",
    "    extras = False\n",
    "    print(\"CUDA NOT supported\")\n",
    "    \n",
    " \n",
    "if not config['checkpoint_training']:\n",
    "    net = Nnet(config['input_dimension'], config['output_dimension']).to(computing_device)\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(),lr = config['learning_rate'])\n",
    "\n",
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = loader(X, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = config['batch_size']\n",
    "validation_split = config['val_split']\n",
    "shuffle_dataset = config['shuffle_dataset']\n",
    "random_seed= 69 \n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, \n",
    "                                               sampler=train_sampler)\n",
    "    \n",
    "validation_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                                sampler=valid_sampler)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN!1!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Track the loss across training\n",
    "total_loss = []\n",
    "avg_minibatch_loss = []\n",
    "num_times_incraesed = 0\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "N = 50\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#TODO: Add dropout! When we do we MUST use model.eval() for val or testing and model.train() for training\n",
    "prev_val_loss = float(\"inf\")\n",
    "\n",
    "for epoch in range(config['epochs']):    \n",
    "    old_net_weights = net.state_dict().copy()\n",
    "    old_optimizer = optimizer.state_dict().copy()\n",
    "    \n",
    "    print(\"Started training epoch : \" + str(checkpoint['epoch'] + 1))\n",
    "    \n",
    "    if checkpoint['epoch'] >= config['epochs']:\n",
    "        break\n",
    "\n",
    "\n",
    "    N_minibatch_loss = 0.0\n",
    "    total_epoch_loss = 0.0\n",
    "    average_epoch_loss = 0.0\n",
    "    num_minibatches = 0\n",
    "    \n",
    "     \n",
    "    net.train() #turns dropout on\n",
    "\n",
    "    # Get the next minibatch of images, labels for training\n",
    "    for minibatch_count, (datapoints, labels) in enumerate(train_loader, 0):\n",
    "\n",
    "        num_minibatches += 1\n",
    "        # Zero out the stored gradient (buffer) from the previous iteration\n",
    "        optimizer.zero_grad()\n",
    "        # Put the minibatch data in CUDA Tensors and run on the GPU if supported\n",
    "        datapoints, labels = datapoints.to(computing_device), labels.to(computing_device)\n",
    "        # Perform the forward pass through the network and compute the loss\n",
    "        outputs = net(datapoints)\n",
    "        \n",
    "        #computing the CEL using the net and the labels\n",
    "        loss = criterion(outputs, labels)\n",
    "        print(loss)\n",
    "        \n",
    "        # Automagically compute the gradients and backpropagate the loss through the network\n",
    "        loss.backward()\n",
    "        total_epoch_loss += loss.item()\n",
    "\n",
    "        # Update the weights\n",
    "        optimizer.step()    \n",
    "        # Add this iteration's loss to the total_loss\n",
    "        total_loss.append(loss.item())\n",
    "        N_minibatch_loss += loss\n",
    "        \n",
    "               \n",
    "        \n",
    "        if minibatch_count % N == 49:\n",
    "            #Print the loss averaged over the last N mini-batches\n",
    "            N_minibatch_loss /= N\n",
    "            print('Epoch %d, average minibatch %d loss: %.3f' % (checkpoint['epoch'] + 1, minibatch_count+1, N_minibatch_loss))\n",
    "            # Add the averaged loss over N minibatches and reset the counter\n",
    "            avg_minibatch_loss.append(N_minibatch_loss)\n",
    "            N_minibatch_loss = 0.0\n",
    "    \n",
    "    average_epoch_loss = total_epoch_loss / num_minibatches\n",
    "    print(\"Finished\", checkpoint['epoch'] + 1, \"epochs of training with average \" + str(average_epoch_loss) + ' loss.' )\n",
    "    \n",
    "    N_minibatch_val_loss = 0\n",
    "    val_data_size = 0\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        net.eval() #turns dropout off\n",
    "        print(\"validation starting: \")\n",
    "\n",
    "        for minibatch_count, (datapoints, labels) in enumerate(validation_loader, 0):\n",
    "            val_data_size += 1\n",
    "            datapoints, labels = images.to(computing_device), labels.to(computing_device)\n",
    "            outputs = net(datapoints)\n",
    "            val_loss = criterion(outputs, labels).item()\n",
    "            N_minibatch_val_loss += val_loss\n",
    "\n",
    "        N_minibatch_val_loss /= val_data_size\n",
    "        print('Epoch ' + str(checkpoint['epoch'] + 1) + ': average validation loss over ' + str(val_data_size)  +\n",
    "              ' datapoints : ' + str(N_minibatch_val_loss) )\n",
    "        \n",
    "        #early stopping\n",
    "        if config['early_stop']:\n",
    "            print(str(N_minibatch_val_loss) + ' vs' + str(prev_val_loss))\n",
    "\n",
    "            if num_times_incraesed >= config['early_stop_epoch']:\n",
    "                print('early stopping triggered')\n",
    "                break\n",
    "            if N_minibatch_val_loss > prev_val_loss:\n",
    "                print('keeping old weights')\n",
    "                num_times_incraesed += 1\n",
    "                net.load_state_dict(old_net_weights)\n",
    "                optimizer.load_state_dict(old_optimizer)\n",
    "            else : \n",
    "                print('val is less than previous')\n",
    "                num_times_incraesed = 0\n",
    "                prev_val_loss = N_minibatch_val_loss\n",
    "\n",
    "             \n",
    "    training_losses.append(average_epoch_loss)\n",
    "    validation_losses.append(N_minibatch_val_loss)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
