{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please read the TODOs as they may be crucial depending on the dataset you are using to train. Configs are not specific to a dataset so choose those as you see appropriate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from baseline_model import *\n",
    "from pathlib import Path \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Path('../../..')\n",
    "\n",
    "config = {}\n",
    "config['k_nearest_baboons'] = 4 #getting the four nearest baboons per frame to consider(MUST BE EVEN)\n",
    "config['k_historic_velocities'] = 4 #getting the four past velocities to consider\n",
    "config['checkpoint_training'] = False\n",
    "config['saved_path'] = 'saved_weights.pth' #path to save current model information (epoch, weights, etc...)\n",
    "config['learning_rate'] = 0.0001 #lr\n",
    "config['batch_size'] = 32 # Number of training samples per batch to be passed to network\n",
    "config['val_split'] = 0.2 #ratio of val and training data\n",
    "config['shuffle_dataset'] = True #shuffle data before start (TODO: change this if we use kfold?)\n",
    "config['epochs'] = 100  # Number of epochs to train the model\n",
    "config['early_stop'] = True  # Implement early stopping or not\n",
    "config['early_stop_epoch'] = 3 # Number of epochs for which validation loss increases to be counted as overfitting\n",
    "config['input_training_data'] = root / 'output/DJI_0870_velocity.csv'\n",
    "config['kmeans_model_path'] = root / 'output' / 'velocity_model.pkl'\n",
    "\n",
    "\n",
    "#DO NOT CHANGE THE FOLLOWING \n",
    "config['input_dimension'] = config['k_nearest_baboons'] + config['k_historic_velocities'] + 1 #DON'T TOUCH THIS\n",
    "config['validation_loss_path'] = '' #leave empty \n",
    "config['training_loss_path'] = '' #leave empty \n",
    "config['output_dimension'] = -1 #leave empty\n",
    "\n",
    "#config checks\n",
    "assert( config['k_nearest_baboons'] % 2 == 0 )\n",
    "assert( os.path.isfile(config['input_training_data']) == True)\n",
    "assert( os.path.isfile(config['kmeans_model_path']) == True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize table for getting nearest baboons and their respective velocities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((2125.4, 1057.18), 51.074506175125435, 10.0),\n",
       " ((2053.7, 939.645), 0.44955044955004064, 11.0),\n",
       " ((2045.13, 905.955), 13.382885251805144, 12.0),\n",
       " ((2007.215, 900.905), 0.6178479958454139, 13.0),\n",
       " ((2161.0550000000007, 941.54), 3.7101154055304586, 14.0),\n",
       " ((2278.565, 1158.07), 6.985331880593899, 15.0),\n",
       " ((2314.695, 1177.34), 12.901044905119257, 16.0),\n",
       " ((2119.07, 809.89), 19.39097995929645, 17.0),\n",
       " ((1908.015, 763.5), 1.8776169959325903, 18.0),\n",
       " ((1923.885, 781.475), 6.434838327744581, 19.0),\n",
       " ((1950.245, 810.35), 1.6483516483603706, 20.0),\n",
       " ((1948.19, 826.1949999999998), 2.1455276318607357, 21.0),\n",
       " ((2105.445, 1237.58), 1.2081305317123932, 22.0),\n",
       " ((2124.105, 1239.65), 0.9477355624850088, 23.0),\n",
       " ((2490.31, 1190.72), 3.146853146853692, 24.0),\n",
       " ((2487.08, 1277.885), 1.0805847978347027, 25.0),\n",
       " ((1707.42, 1122.74), 0.14985014984660636, 26.0),\n",
       " ((1785.4699999999998, 1085.1), 15.050804630763162, 27.0),\n",
       " ((1800.63, 1090.315), 3.8526354042502913, 28.0),\n",
       " ((1865.715, 1096.415), 2.5781194606482063, 29.0),\n",
       " ((1936.615, 1130.0749999999996), 2.4393837057189582, 30.0),\n",
       " ((1836.705, 1101.535), 0.9477355624957832, 31.0),\n",
       " ((1514.1950000000004, 1070.125), 14.424585076251118, 32.0),\n",
       " ((1708.935, 1301.365), 1.0596005712115035, 33.0),\n",
       " ((2106.57, 1522.23), 21.45998549660368, 34.0),\n",
       " ((2046.85, 1537.89), 5.310702841163275, 35.0),\n",
       " ((2721.89, 1494.69), 0.6701502430032729, 36.0),\n",
       " ((2202.05, 1108.05), 0.2997002997068415, 37.0),\n",
       " ((2227.535, 1120.34), 0.9115028766610744, 38.0),\n",
       " ((2031.965, 816.7249999999998), 1.5281776764053896, 39.0),\n",
       " ((2057.76, 848.5), 1.5644814948393388, 40.0),\n",
       " ((2080.02, 859.9449999999998), 1.613935506635413, 41.0)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This is assuming the obvious assumption that a baboon only has one position per frame\n",
    "#TODO : Assuming data is ordered by frame (increasing timesteps/frames)\n",
    "training_data = pd.read_csv(config['input_training_data'])\n",
    "\n",
    "frames_to_velocities = {} #Maps a single frame to a list of all the baboons, their id, position, and velocity\n",
    "\n",
    "for idx, row in training_data.iterrows():\n",
    "    current_frame = row.loc['frame']\n",
    "    current_position = (row.loc['centroid_x'], row.loc['centroid_y'])\n",
    "    current_velocity = row.loc['velocity']\n",
    "    current_id = row.loc['baboon id']\n",
    "    \n",
    "    if current_frame not in frames_to_velocities:\n",
    "        frames_to_velocities[current_frame] = []\n",
    "    \n",
    "    frames_to_velocities[current_frame].append((current_position, current_velocity, current_id))\n",
    "\n",
    "\"\"\"\n",
    "Returns the K physically nearest baboons to baboon_id at the given frame\n",
    "\n",
    "Pre-pends with negative ones if there are less than k nearest baboons in the frame\n",
    "\"\"\"\n",
    "def get_k_nearest_baboons_velocities(frame, baboon_id):\n",
    "    baboons_in_frame = frames_to_velocities[frame].copy()\n",
    "    output = []\n",
    "    \n",
    "    target_position = (-1, -1)\n",
    "    \n",
    "    #get rid of the baboon we are considering\n",
    "    for idx, baboon in enumerate(baboons_in_frame):\n",
    "        if baboon[2] == baboon_id:\n",
    "            target_velocity = baboons_in_frame[idx][0]\n",
    "            del baboons_in_frame[idx]\n",
    "            break\n",
    "      \n",
    "    if len(baboons_in_frame) <= config['k_nearest_baboons']:\n",
    "        padded_output = list(np.ones(config['k_nearest_baboons'] - len(baboons_in_frame)) * -1)\n",
    "        return padded_output.extend([item[1] for item in baboons_in_frame])\n",
    "    \n",
    "    for k_nearest_neighbor in range(config['k_nearest_baboons']):\n",
    "        min_distance = math.inf\n",
    "        min_velocity = -1\n",
    "        min_idx = -1\n",
    "        \n",
    "        for idx, baboon in enumerate(baboons_in_frame):\n",
    "            current_distance = np.linalg.norm(np.array(baboon[0]) - np.array(target_velocity))\n",
    "            if current_distance < min_distance:\n",
    "                min_distance = current_distance\n",
    "                min_velocity = baboon[1]\n",
    "                min_idx = idx\n",
    "        \n",
    "        output.append(min_velocity)\n",
    "        del baboons_in_frame[min_idx]\n",
    "        \n",
    "    return output\n",
    "            \n",
    "frames_to_velocities[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.2997002997068415,\n",
       " 0.9115028766610744,\n",
       " 3.7101154055304586,\n",
       " 0.44955044955004064]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_k_nearest_baboons_velocities(1, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To get the historic velocities per baboon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(12.85046947525376, 21.0),\n",
       " (12.97826878451604, 22.0),\n",
       " (12.644369496333105, 23.0),\n",
       " (12.850469475230561, 24.0),\n",
       " (12.85046947525376, 25.0),\n",
       " (12.85046947522877, 26.0),\n",
       " (12.850469475230561, 27.0),\n",
       " (12.850469475242162, 28.0),\n",
       " (12.978268784517812, 29.0),\n",
       " (12.723151376903115, 30.0),\n",
       " (12.85046947522877, 31.0),\n",
       " (12.978268784519587, 32.0),\n",
       " (12.850469475240375, 33.0),\n",
       " (12.723151376903115, 34.0),\n",
       " (12.978268784517812, 35.0),\n",
       " (12.85046947522877, 36.0),\n",
       " (12.723151376891556, 37.0),\n",
       " (12.85046947525376, 38.0),\n",
       " (12.97826878451604, 39.0),\n",
       " (12.850469475232348, 40.0),\n",
       " (12.723151376903115, 41.0),\n",
       " (12.978268784517812, 42.0),\n",
       " (12.850469475240375, 43.0),\n",
       " (12.850469475230561, 44.0),\n",
       " (12.850469475230561, 45.0),\n",
       " (12.850469475240375, 46.0),\n",
       " (12.85046947524395, 47.0),\n",
       " (12.85046947522877, 48.0),\n",
       " (12.850469475230561, 49.0),\n",
       " (12.850469475242162, 50.0),\n",
       " (50.51439438630578, 51.0),\n",
       " (50.56171446434248, 52.0),\n",
       " (50.51439438631872, 53.0),\n",
       " (50.41962099716843, 54.0),\n",
       " (50.41962099717059, 55.0),\n",
       " (50.51439438631872, 56.0),\n",
       " (50.56171446434248, 57.0),\n",
       " (50.51439438630578, 58.0),\n",
       " (50.56171446434248, 59.0),\n",
       " (50.56171446434033, 60.0),\n",
       " (22.147933257394644, 61.0),\n",
       " (22.43602563139897, 62.0),\n",
       " (22.436025631385856, 63.0),\n",
       " (22.4360256313999, 64.0),\n",
       " (22.1479332573937, 65.0),\n",
       " (22.436025631385856, 66.0),\n",
       " (22.436025631385856, 67.0),\n",
       " (22.436025631400828, 68.0),\n",
       " (22.147933257392754, 69.0),\n",
       " (22.436025631385856, 70.0),\n",
       " (11.564704124815332, 71.0),\n",
       " (11.722841487875636, 72.0),\n",
       " (11.436816325528085, 73.0),\n",
       " (11.722841487875636, 74.0),\n",
       " (11.436816325535226, 75.0),\n",
       " (11.722841487878512, 76.0),\n",
       " (11.564704124818244, 77.0),\n",
       " (11.596697750618631, 78.0),\n",
       " (11.564704124815332, 79.0),\n",
       " (11.596697750618631, 80.0),\n",
       " (11.564704124822395, 81.0),\n",
       " (11.722841487875636, 82.0),\n",
       " (11.436816325528085, 83.0),\n",
       " (11.722841487875636, 84.0),\n",
       " (11.436816325528085, 85.0),\n",
       " (11.722841487881384, 86.0),\n",
       " (11.564704124822395, 87.0),\n",
       " (11.596697750618631, 88.0),\n",
       " (11.564704124815332, 89.0),\n",
       " (11.596697750618631, 90.0),\n",
       " (18.354043731199216, 91.0),\n",
       " (18.354043731185968, 92.0),\n",
       " (18.17330821474484, 93.0),\n",
       " (18.499667088826754, 94.0),\n",
       " (18.20848896285627, 95.0),\n",
       " (18.465041119775933, 96.0),\n",
       " (18.20848896284303, 97.0),\n",
       " (18.499667088840003, 98.0),\n",
       " (18.17330821473157, 99.0),\n",
       " (18.20848896285627, 100.0),\n",
       " (45.29863701043879, 101.0),\n",
       " (45.29863701043879, 102.0),\n",
       " (45.44488375871215, 103.0),\n",
       " (45.29863701043844, 104.0),\n",
       " (45.26615624473773, 105.0),\n",
       " (45.26615624473107, 106.0),\n",
       " (45.29863701043844, 107.0),\n",
       " (45.298637010442114, 108.0),\n",
       " (45.44488375871215, 109.0),\n",
       " (45.298637010435456, 110.0),\n",
       " (0.0, 111.0),\n",
       " (0.0, 112.0),\n",
       " (0.0, 113.0),\n",
       " (0.0, 114.0),\n",
       " (0.0, 115.0),\n",
       " (0.0, 116.0),\n",
       " (0.0, 117.0),\n",
       " (0.0, 118.0),\n",
       " (0.0, 119.0),\n",
       " (0.0, 120.0),\n",
       " (0.0, 121.0),\n",
       " (0.0, 122.0),\n",
       " (0.0, 123.0),\n",
       " (0.0, 124.0),\n",
       " (0.0, 125.0),\n",
       " (0.0, 126.0),\n",
       " (0.0, 127.0),\n",
       " (0.0, 128.0),\n",
       " (0.0, 129.0),\n",
       " (0.0, 130.0)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO : Must check for 'extended' periods of discountinuous frames for future potentially sparese labeled data\n",
    "#TODO : Assuming data is ordered by frame (increasing timesteps/frames)\n",
    "#TODO : Warning : if there are not enough K historic frames then we pre pad with negative ones\n",
    "\n",
    "baboons_to_velocities = {} #maps the baboon's id to it's velocties and respective frames in a tuple\n",
    "\n",
    "for idx, row in training_data.iterrows():\n",
    "    current_frame = row.loc['frame']\n",
    "    current_position = (row.loc['centroid_x'], row.loc['centroid_y'])\n",
    "    current_velocity = row.loc['velocity']\n",
    "    current_id = row.loc['baboon id']\n",
    "    \n",
    "    if current_id not in baboons_to_velocities:\n",
    "        baboons_to_velocities[current_id] = []\n",
    "        \n",
    "    baboons_to_velocities[current_id].append((current_velocity, current_frame))\n",
    "\n",
    "\"\"\"\n",
    "Given a frame returns the k past velocities of the given baboon_id\n",
    "\n",
    "Pre pads with negative ones if there are less than k previous frames available in the labeled data\n",
    "\"\"\"\n",
    "def get_k_past_velocities(current_frame, baboon_id):\n",
    "    velocities = baboons_to_velocities[baboon_id]\n",
    "    \n",
    "    #annoying but I don't know how to do this easier\n",
    "    frame_index = -1\n",
    "    for idx, frame in enumerate(velocities):\n",
    "        if frame[1] == current_frame:\n",
    "            frame_index = idx\n",
    "            break\n",
    "            \n",
    "    if frame_index == -1:\n",
    "        raise RuntimeError('Frame does not exist in dataset for this baboon_id')\n",
    "        \n",
    "    if frame_index < config['k_historic_velocities']:\n",
    "        padded_output = list(np.ones(config['k_historic_velocities'] - frame_index) * -1)   \n",
    "        padded_output.extend([item[0] for item in velocities[:frame_index]])\n",
    "        return padded_output\n",
    "    \n",
    "    else:\n",
    "        return [item[0] for item in velocities[frame_index-config['k_historic_velocities'] : frame_index]]\n",
    "    \n",
    "baboons_to_velocities[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1.0, -1.0, -1.0, 12.85046947525376]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_k_past_velocities(22, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Data and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  3.76458762,  14.74763297,  35.8249973 ,  83.11665914,\n",
       "       407.57334622])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO : way too hard coded here. Make better\n",
    "\n",
    "training_data = pd.read_csv(config['input_training_data'])\n",
    "\n",
    "kmeans = pickle.load(open(config['kmeans_model_path'], 'rb'))\n",
    "kmeans_centers_sorted = np.sort(np.array(kmeans.cluster_centers_).squeeze())\n",
    "config['output_dimension'] = kmeans_centers_sorted.size + 1\n",
    "\n",
    "# test_predict = np.array([10])\n",
    "# print(kmeans.predict(test_predict.reshape(-1,1)))\n",
    "#TODO : add the sitting still !!!\n",
    "label_table = {}\n",
    "label_table[0] = [1,0,0,0,0,0] #sitting\n",
    "label_table[kmeans_centers_sorted[0]] = [0,1,0,0,0,0]\n",
    "label_table[kmeans_centers_sorted[1]] = [0,0,1,0,0,0]\n",
    "label_table[kmeans_centers_sorted[2]] = [0,0,0,1,0,0]\n",
    "label_table[kmeans_centers_sorted[3]] = [0,0,0,0,1,0]\n",
    "label_table[kmeans_centers_sorted[4]] = [0,0,0,0,0,1]\n",
    "\n",
    "\n",
    "kmeans_centers_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12.85046948, 53.82624892, 27.9806787 ,  1.47585282,  1.52817768,\n",
       "       -1.        , -1.        , -1.        , -1.        ])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# [ target v, k nearest baboon's v, k' past velocities ]\n",
    "# X = np.zeros((training_data.shape[0], config['input_dimension']))\n",
    "# labels = np.zeros(())\n",
    "X = []\n",
    "labels = []\n",
    "\n",
    "start_historic = 1 + config['k_nearest_baboons']\n",
    "\n",
    "\n",
    "for idx, row in training_data.iterrows():\n",
    "    current_frame = row.loc['frame']\n",
    "    current_position = (row.loc['centroid_x'], row.loc['centroid_y'])\n",
    "    current_velocity = row.loc['velocity']\n",
    "    current_id = row.loc['baboon id']\n",
    "    \n",
    "    current_label = training_data.loc[(training_data['frame'] == (current_frame + 1)) & (training_data['baboon id'] == current_id)]\n",
    "    if(current_label.empty):\n",
    "        #last frame for baboon, therefore no label\n",
    "        continue\n",
    "        \n",
    "    #create the label\n",
    "    current_one_hot = []\n",
    "    current_label_velocity = current_label['velocity'].to_numpy()\n",
    "    if current_label_velocity[0] == 0:\n",
    "        current_one_hot = label_table[0]\n",
    "    else:\n",
    "        kmeans_label = kmeans.predict(current_label_velocity.reshape(-1,1))[0]\n",
    "        current_one_hot = label_table[ kmeans.cluster_centers_[kmeans_label][0] ]\n",
    "    current_one_hot = np.array(current_one_hot)\n",
    "    \n",
    "    #create the datapoint\n",
    "    current_datapoint = np.zeros(config['input_dimension'])\n",
    "    current_datapoint[0] = current_velocity\n",
    "    current_datapoint[1:start_historic] = get_k_nearest_baboons_velocities(current_frame, current_id)\n",
    "    current_datapoint[start_historic:] = get_k_past_velocities(current_frame, current_id)\n",
    "    \n",
    "    X.append(current_datapoint)\n",
    "    labels.append(current_one_hot)\n",
    "\n",
    "\n",
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((52990, 9), (52990, 6))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array(X)\n",
    "labels = np.array(labels)\n",
    "\n",
    "X.shape, labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA NOT supported\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Nnet(\n",
       "  (input_layer): Sequential(\n",
       "    (0): Linear(in_features=9, out_features=500, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (main): Sequential(\n",
       "    (0): Linear(in_features=500, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=25088, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Dropout(p=0.5, inplace=False)\n",
       "    (9): Linear(in_features=4096, out_features=500, bias=True)\n",
       "    (10): ReLU(inplace=True)\n",
       "    (11): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (output): Linear(in_features=500, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO : add weighted classes to save from an unbalanced dataset\n",
    "# Check if your system supports CUDA\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "# Setup GPU optimization if CUDA is supported\n",
    "if use_cuda:\n",
    "    computing_device = torch.device(\"cuda\")\n",
    "    extras = {\"num_workers\": 1, \"pin_memory\": True}\n",
    "    print(\"CUDA is supported\")\n",
    "else: # Otherwise, train on the CPU\n",
    "    computing_device = torch.device(\"cpu\")\n",
    "    extras = False\n",
    "    print(\"CUDA NOT supported\")\n",
    "    \n",
    " \n",
    "if not config['checkpoint_training']:\n",
    "    net = Nnet(config['input_dimension'], config['output_dimension']).to(computing_device)\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(),lr = config['learning_rate'])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = loader(X, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = config['batch_size']\n",
    "validation_split = config['val_split']\n",
    "shuffle_dataset = config['shuffle_dataset']\n",
    "random_seed= 69 \n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, \n",
    "                                               sampler=train_sampler)\n",
    "    \n",
    "validation_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                                sampler=valid_sampler)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN!1!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started training epoch : 0\n",
      "Minibatch 0 / 42 loss : 0.47447651624679565\n",
      "Minibatch 1 / 42 loss : 1.6009562015533447\n",
      "Minibatch 2 / 42 loss : 0.2548975348472595\n",
      "Minibatch 3 / 42 loss : 0.8573656678199768\n",
      "Minibatch 4 / 42 loss : 1.0877360105514526\n",
      "Minibatch 5 / 42 loss : 1.287022352218628\n",
      "Minibatch 6 / 42 loss : 0.46928390860557556\n",
      "Minibatch 7 / 42 loss : 0.33837148547172546\n",
      "Minibatch 8 / 42 loss : 0.4740034341812134\n",
      "Minibatch 9 / 42 loss : 0.5740264058113098\n",
      "Minibatch 10 / 42 loss : 0.7563921213150024\n",
      "Minibatch 11 / 42 loss : 0.5426720976829529\n",
      "Minibatch 12 / 42 loss : 0.6197271347045898\n",
      "Minibatch 13 / 42 loss : 0.674466609954834\n",
      "Minibatch 14 / 42 loss : 0.9123842120170593\n",
      "Minibatch 15 / 42 loss : 0.8851945996284485\n",
      "Minibatch 16 / 42 loss : 0.4961041212081909\n",
      "Minibatch 17 / 42 loss : 0.7268228530883789\n",
      "Minibatch 18 / 42 loss : 0.8990293145179749\n",
      "Minibatch 19 / 42 loss : 0.309673011302948\n",
      "Minibatch 20 / 42 loss : 0.7001256346702576\n",
      "Minibatch 21 / 42 loss : 0.6724161505699158\n",
      "Minibatch 22 / 42 loss : 0.648097813129425\n",
      "Minibatch 23 / 42 loss : 0.4752207100391388\n",
      "Minibatch 24 / 42 loss : 0.8161076903343201\n",
      "Minibatch 25 / 42 loss : 0.7467178106307983\n",
      "Minibatch 26 / 42 loss : 0.4938063621520996\n",
      "Minibatch 27 / 42 loss : 0.6887941360473633\n",
      "Minibatch 28 / 42 loss : 0.8590289950370789\n",
      "Minibatch 29 / 42 loss : 0.8676438927650452\n",
      "Minibatch 30 / 42 loss : 0.4954583942890167\n",
      "Minibatch 31 / 42 loss : 1.110007405281067\n",
      "Minibatch 32 / 42 loss : 0.4453074038028717\n",
      "Minibatch 33 / 42 loss : 0.47294482588768005\n",
      "Minibatch 34 / 42 loss : 0.5724756717681885\n",
      "Minibatch 35 / 42 loss : 0.8538892269134521\n",
      "Minibatch 36 / 42 loss : 0.7613198161125183\n",
      "Minibatch 37 / 42 loss : 0.559648871421814\n",
      "Minibatch 38 / 42 loss : 0.7391664981842041\n",
      "Minibatch 39 / 42 loss : 0.49243244528770447\n",
      "Minibatch 40 / 42 loss : 0.7620907425880432\n",
      "Minibatch 41 / 42 loss : 0.8072242140769958\n",
      "Minibatch 42 / 42 loss : 0.5893240571022034\n",
      "Minibatch 43 / 42 loss : 0.41009894013404846\n",
      "Minibatch 44 / 42 loss : 0.638480544090271\n",
      "Minibatch 45 / 42 loss : 0.8060619831085205\n",
      "Minibatch 46 / 42 loss : 0.6537317633628845\n",
      "Minibatch 47 / 42 loss : 1.0074260234832764\n",
      "Minibatch 48 / 42 loss : 0.5604286193847656\n",
      "Minibatch 49 / 42 loss : 0.36342495679855347\n",
      "Epoch 1, average minibatch 50 loss: 0.6861901879310608\n",
      "Minibatch 50 / 42 loss : 2.3554790019989014\n",
      "Minibatch 51 / 42 loss : 0.6580967307090759\n",
      "Minibatch 52 / 42 loss : 0.5605624318122864\n",
      "Minibatch 53 / 42 loss : 1.103796362876892\n",
      "Minibatch 54 / 42 loss : 0.42308691143989563\n",
      "Minibatch 55 / 42 loss : 0.5913412570953369\n",
      "Minibatch 56 / 42 loss : 0.3304829001426697\n",
      "Minibatch 57 / 42 loss : 0.5854225754737854\n",
      "Minibatch 58 / 42 loss : 0.9254053235054016\n",
      "Minibatch 59 / 42 loss : 0.6938915848731995\n",
      "Minibatch 60 / 42 loss : 0.34710827469825745\n",
      "Minibatch 61 / 42 loss : 0.4169706702232361\n",
      "Minibatch 62 / 42 loss : 0.6590896844863892\n",
      "Minibatch 63 / 42 loss : 0.4812357425689697\n",
      "Minibatch 64 / 42 loss : 0.5588310360908508\n",
      "Minibatch 65 / 42 loss : 1.1530191898345947\n",
      "Minibatch 66 / 42 loss : 0.7399833798408508\n",
      "Minibatch 67 / 42 loss : 0.7126114964485168\n",
      "Minibatch 68 / 42 loss : 0.9145629405975342\n",
      "Minibatch 69 / 42 loss : 0.4080820381641388\n",
      "Minibatch 70 / 42 loss : 0.641583263874054\n",
      "Minibatch 71 / 42 loss : 0.4940152168273926\n",
      "Minibatch 72 / 42 loss : 0.48336341977119446\n",
      "Minibatch 73 / 42 loss : 0.4271445870399475\n",
      "Minibatch 74 / 42 loss : 0.9866677522659302\n",
      "Minibatch 75 / 42 loss : 0.6517301797866821\n",
      "Minibatch 76 / 42 loss : 1.4697165489196777\n",
      "Minibatch 77 / 42 loss : 0.4147535264492035\n",
      "Minibatch 78 / 42 loss : 0.4542258679866791\n",
      "Minibatch 79 / 42 loss : 0.4401402175426483\n",
      "Minibatch 80 / 42 loss : 0.5158736109733582\n",
      "Minibatch 81 / 42 loss : 0.35000431537628174\n",
      "Minibatch 82 / 42 loss : 0.7158255577087402\n",
      "Minibatch 83 / 42 loss : 0.47576552629470825\n",
      "Minibatch 84 / 42 loss : 0.620345950126648\n",
      "Minibatch 85 / 42 loss : 0.6238546371459961\n",
      "Minibatch 86 / 42 loss : 0.5516968965530396\n",
      "Minibatch 87 / 42 loss : 0.37069782614707947\n",
      "Minibatch 88 / 42 loss : 0.5833389163017273\n",
      "Minibatch 89 / 42 loss : 0.7392663359642029\n",
      "Minibatch 90 / 42 loss : 0.6969839930534363\n",
      "Minibatch 91 / 42 loss : 0.9270662069320679\n",
      "Minibatch 92 / 42 loss : 0.5030547976493835\n",
      "Minibatch 93 / 42 loss : 1.2007440328598022\n",
      "Minibatch 94 / 42 loss : 0.8954442739486694\n",
      "Minibatch 95 / 42 loss : 0.6605983972549438\n",
      "Minibatch 96 / 42 loss : 1.083039402961731\n",
      "Minibatch 97 / 42 loss : 0.5063462257385254\n",
      "Minibatch 98 / 42 loss : 0.8839477300643921\n",
      "Minibatch 99 / 42 loss : 0.6564075946807861\n",
      "Epoch 1, average minibatch 100 loss: 0.692854106426239\n",
      "Minibatch 100 / 42 loss : 0.680025577545166\n",
      "Minibatch 101 / 42 loss : 0.6809679269790649\n",
      "Minibatch 102 / 42 loss : 0.7506815195083618\n",
      "Minibatch 103 / 42 loss : 0.49666956067085266\n",
      "Minibatch 104 / 42 loss : 1.1576080322265625\n",
      "Minibatch 105 / 42 loss : 0.6799278855323792\n",
      "Minibatch 106 / 42 loss : 0.9596764445304871\n",
      "Minibatch 107 / 42 loss : 0.6226812601089478\n",
      "Minibatch 108 / 42 loss : 0.7194423079490662\n",
      "Minibatch 109 / 42 loss : 0.6738144755363464\n",
      "Minibatch 110 / 42 loss : 0.7518950700759888\n",
      "Minibatch 111 / 42 loss : 0.6071733832359314\n",
      "Minibatch 112 / 42 loss : 0.6075056791305542\n",
      "Minibatch 113 / 42 loss : 0.885484516620636\n",
      "Minibatch 114 / 42 loss : 0.7881311178207397\n",
      "Minibatch 115 / 42 loss : 0.44142740964889526\n",
      "Minibatch 116 / 42 loss : 0.590543270111084\n",
      "Minibatch 117 / 42 loss : 0.6180247068405151\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-e108e891b9c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# Update the weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;31m# Add this iteration's loss to the total_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mtotal_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    105\u001b[0m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Track the loss across training\n",
    "total_loss = []\n",
    "avg_minibatch_loss = []\n",
    "num_times_incraesed = 0\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "N = 50\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#TODO: Add dropout! When we do we MUST use model.eval() for val or testing and model.train() for training\n",
    "prev_val_loss = float(\"inf\")\n",
    "\n",
    "for epoch in range(config['epochs']):    \n",
    "    old_net_weights = net.state_dict().copy()\n",
    "    old_optimizer = optimizer.state_dict().copy()\n",
    "    \n",
    "    print(f\"Started training epoch : {epoch}\" )\n",
    "    \n",
    "\n",
    "    N_minibatch_loss = 0.0\n",
    "    total_epoch_loss = 0.0\n",
    "    average_epoch_loss = 0.0\n",
    "    num_minibatches = 0\n",
    "    \n",
    "     \n",
    "    net.train() #turns dropout on\n",
    "\n",
    "    # Get the next minibatch of images, labels for training\n",
    "    for minibatch_count, (datapoints, labels) in enumerate(train_loader, 0):\n",
    "\n",
    "        num_minibatches += 1\n",
    "        # Zero out the stored gradient (buffer) from the previous iteration\n",
    "        optimizer.zero_grad()\n",
    "        # Put the minibatch data in CUDA Tensors and run on the GPU if supported\n",
    "        datapoints, labels = datapoints.to(computing_device), labels.to(computing_device)\n",
    "        # Perform the forward pass through the network and compute the loss\n",
    "        outputs = net(datapoints.float())\n",
    "        labels = torch.max(labels, 1)[1]\n",
    "        \n",
    "        #computing the CEL using the net and the labels\n",
    "        loss = criterion(outputs, labels)\n",
    "        print(f'Minibatch {minibatch_count} loss : {loss.item()}')\n",
    "        \n",
    "        # Automagically compute the gradients and backpropagate the loss through the network\n",
    "        loss.backward()\n",
    "        total_epoch_loss += loss.item()\n",
    "\n",
    "        # Update the weights\n",
    "        optimizer.step()    \n",
    "        # Add this iteration's loss to the total_loss\n",
    "        total_loss.append(loss.item())\n",
    "        N_minibatch_loss += loss\n",
    "        \n",
    "               \n",
    "        \n",
    "        if minibatch_count % N == 49:\n",
    "            #Print the loss averaged over the last N mini-batches\n",
    "            N_minibatch_loss /= N\n",
    "            print(f'Epoch {epoch + 1}, average minibatch {minibatch_count+1} loss: {N_minibatch_loss}')\n",
    "            # Add the averaged loss over N minibatches and reset the counter\n",
    "            avg_minibatch_loss.append(N_minibatch_loss)\n",
    "            N_minibatch_loss = 0.0\n",
    "    \n",
    "    average_epoch_loss = total_epoch_loss / num_minibatches\n",
    "    print(f\"Finished {epoch+ 1} epochs of training with average {average_epoch_loss} loss.\" )\n",
    "    \n",
    "    N_minibatch_val_loss = 0\n",
    "    val_data_size = 0\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        net.eval() #turns dropout off\n",
    "        print(\"validation starting: \")\n",
    "\n",
    "        for minibatch_count, (datapoints, labels) in enumerate(validation_loader, 0):\n",
    "            val_data_size += 1\n",
    "            datapoints, labels = images.to(computing_device), labels.to(computing_device)\n",
    "            outputs = net(datapoints)\n",
    "            val_loss = criterion(outputs, labels).item()\n",
    "            N_minibatch_val_loss += val_loss\n",
    "\n",
    "        N_minibatch_val_loss /= val_data_size\n",
    "        print(f'Epoch {epoch + 1} average validation loss over {val_data_size} datapoints : {N_minibatch_val_loss}' )\n",
    "        \n",
    "        #early stopping\n",
    "        if config['early_stop']:\n",
    "            print(str(N_minibatch_val_loss) + ' vs' + str(prev_val_loss))\n",
    "\n",
    "            if num_times_incraesed >= config['early_stop_epoch']:\n",
    "                print('early stopping triggered')\n",
    "                break\n",
    "            if N_minibatch_val_loss > prev_val_loss:\n",
    "                print('keeping old weights')\n",
    "                num_times_incraesed += 1\n",
    "                net.load_state_dict(old_net_weights)\n",
    "                optimizer.load_state_dict(old_optimizer)\n",
    "            else : \n",
    "                print('val is less than previous')\n",
    "                num_times_incraesed = 0\n",
    "                prev_val_loss = N_minibatch_val_loss\n",
    "\n",
    "             \n",
    "    training_losses.append(average_epoch_loss)\n",
    "    validation_losses.append(N_minibatch_val_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([], [])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_losses, validation_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'epochs_plots' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-1e2611c72932>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs_plots\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_plots\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs_plots\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_plots\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'upper right'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'epochs_plots' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "min_entries = min(len(training_losses), len(validation_losses))\n",
    "for i in range(min_entries):\n",
    "    epochs_plots.append(i+1)\n",
    "    \n",
    "val_plots = training_losses\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs_plots, val_plots, label='val loss')\n",
    "plt.plot(epochs_plots, train_plots, label='train loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('cross entropy error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
