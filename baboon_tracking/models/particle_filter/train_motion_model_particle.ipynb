{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please read the TODOs as they may be crucial depending on the dataset you are using to train. Configs are not specific to a dataset so choose those as you see appropriate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from baseline_model import *\n",
    "from pathlib import Path \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Path('../../..')\n",
    "\n",
    "config = {}\n",
    "config['k_nearest_baboons'] = 4 #getting the four nearest baboons per frame to consider(MUST BE EVEN)\n",
    "config['k_historic_velocities'] = 4 #getting the four past velocities to consider\n",
    "config['checkpoint_training'] = False\n",
    "config['saved_path'] = 'saved_weights.pth' #path to save current model information (epoch, weights, etc...)\n",
    "config['learning_rate'] = 0.0001 #lr\n",
    "config['batch_size'] = 32 # Number of training samples per batch to be passed to network\n",
    "config['val_split'] = 0.2 #ratio of val and training data\n",
    "config['shuffle_dataset'] = True #shuffle data before start (TODO: change this if we use kfold?)\n",
    "config['epochs'] = 100  # Number of epochs to train the model\n",
    "config['early_stop'] = True  # Implement early stopping or not\n",
    "config['early_stop_epoch'] = 3 # Number of epochs for which validation loss increases to be counted as overfitting\n",
    "config['input_training_data'] = root / 'output/DJI_0870_velocity.csv'\n",
    "config['kmeans_model_path'] = root / 'output' / 'velocity_model.pkl'\n",
    "\n",
    "\n",
    "#DO NOT CHANGE THE FOLLOWING \n",
    "config['input_dimension'] = config['k_nearest_baboons'] + config['k_historic_velocities'] + 1 #DON'T TOUCH THIS\n",
    "config['validation_loss_path'] = '' #leave empty \n",
    "config['training_loss_path'] = '' #leave empty \n",
    "config['output_dimension'] = -1 #leave empty\n",
    "\n",
    "#config checks\n",
    "assert( config['k_nearest_baboons'] % 2 == 0 )\n",
    "assert( os.path.isfile(config['input_training_data']) == True)\n",
    "assert( os.path.isfile(config['kmeans_model_path']) == True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize table for getting nearest baboons and their respective velocities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((2125.4, 1057.18), 51.074506175125435, 10.0),\n",
       " ((2053.7, 939.645), 0.44955044955004064, 11.0),\n",
       " ((2045.13, 905.955), 13.382885251805144, 12.0),\n",
       " ((2007.215, 900.905), 0.6178479958454139, 13.0),\n",
       " ((2161.0550000000007, 941.54), 3.7101154055304586, 14.0),\n",
       " ((2278.565, 1158.07), 6.985331880593899, 15.0),\n",
       " ((2314.695, 1177.34), 12.901044905119257, 16.0),\n",
       " ((2119.07, 809.89), 19.39097995929645, 17.0),\n",
       " ((1908.015, 763.5), 1.8776169959325903, 18.0),\n",
       " ((1923.885, 781.475), 6.434838327744581, 19.0),\n",
       " ((1950.245, 810.35), 1.6483516483603706, 20.0),\n",
       " ((1948.19, 826.1949999999998), 2.1455276318607357, 21.0),\n",
       " ((2105.445, 1237.58), 1.2081305317123932, 22.0),\n",
       " ((2124.105, 1239.65), 0.9477355624850088, 23.0),\n",
       " ((2490.31, 1190.72), 3.146853146853692, 24.0),\n",
       " ((2487.08, 1277.885), 1.0805847978347027, 25.0),\n",
       " ((1707.42, 1122.74), 0.14985014984660636, 26.0),\n",
       " ((1785.4699999999998, 1085.1), 15.050804630763162, 27.0),\n",
       " ((1800.63, 1090.315), 3.8526354042502913, 28.0),\n",
       " ((1865.715, 1096.415), 2.5781194606482063, 29.0),\n",
       " ((1936.615, 1130.0749999999996), 2.4393837057189582, 30.0),\n",
       " ((1836.705, 1101.535), 0.9477355624957832, 31.0),\n",
       " ((1514.1950000000004, 1070.125), 14.424585076251118, 32.0),\n",
       " ((1708.935, 1301.365), 1.0596005712115035, 33.0),\n",
       " ((2106.57, 1522.23), 21.45998549660368, 34.0),\n",
       " ((2046.85, 1537.89), 5.310702841163275, 35.0),\n",
       " ((2721.89, 1494.69), 0.6701502430032729, 36.0),\n",
       " ((2202.05, 1108.05), 0.2997002997068415, 37.0),\n",
       " ((2227.535, 1120.34), 0.9115028766610744, 38.0),\n",
       " ((2031.965, 816.7249999999998), 1.5281776764053896, 39.0),\n",
       " ((2057.76, 848.5), 1.5644814948393388, 40.0),\n",
       " ((2080.02, 859.9449999999998), 1.613935506635413, 41.0)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This is assuming the obvious assumption that a baboon only has one position per frame\n",
    "#TODO : Assuming data is ordered by frame (increasing timesteps/frames)\n",
    "training_data = pd.read_csv(config['input_training_data'])\n",
    "\n",
    "frames_to_velocities = {} #Maps a single frame to a list of all the baboons, their id, position, and velocity\n",
    "\n",
    "for idx, row in training_data.iterrows():\n",
    "    current_frame = row.loc['frame']\n",
    "    current_position = (row.loc['centroid_x'], row.loc['centroid_y'])\n",
    "    current_velocity = row.loc['velocity']\n",
    "    current_id = row.loc['baboon id']\n",
    "    \n",
    "    if current_frame not in frames_to_velocities:\n",
    "        frames_to_velocities[current_frame] = []\n",
    "    \n",
    "    frames_to_velocities[current_frame].append((current_position, current_velocity, current_id))\n",
    "\n",
    "\"\"\"\n",
    "Returns the K physically nearest baboons to baboon_id at the given frame\n",
    "\n",
    "Pre-pends with negative ones if there are less than k nearest baboons in the frame\n",
    "\"\"\"\n",
    "def get_k_nearest_baboons_velocities(frame, baboon_id):\n",
    "    baboons_in_frame = frames_to_velocities[frame].copy()\n",
    "    output = []\n",
    "    \n",
    "    target_position = (-1, -1)\n",
    "    \n",
    "    #get rid of the baboon we are considering\n",
    "    for idx, baboon in enumerate(baboons_in_frame):\n",
    "        if baboon[2] == baboon_id:\n",
    "            target_velocity = baboons_in_frame[idx][0]\n",
    "            del baboons_in_frame[idx]\n",
    "            break\n",
    "      \n",
    "    if len(baboons_in_frame) <= config['k_nearest_baboons']:\n",
    "        padded_output = list(np.ones(config['k_nearest_baboons'] - len(baboons_in_frame)) * -1)\n",
    "        return padded_output.extend([item[1] for item in baboons_in_frame])\n",
    "    \n",
    "    for k_nearest_neighbor in range(config['k_nearest_baboons']):\n",
    "        min_distance = math.inf\n",
    "        min_velocity = -1\n",
    "        min_idx = -1\n",
    "        \n",
    "        for idx, baboon in enumerate(baboons_in_frame):\n",
    "            current_distance = np.linalg.norm(np.array(baboon[0]) - np.array(target_velocity))\n",
    "            if current_distance < min_distance:\n",
    "                min_distance = current_distance\n",
    "                min_velocity = baboon[1]\n",
    "                min_idx = idx\n",
    "        \n",
    "        output.append(min_velocity)\n",
    "        del baboons_in_frame[min_idx]\n",
    "        \n",
    "    return output\n",
    "            \n",
    "frames_to_velocities[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.2997002997068415,\n",
       " 0.9115028766610744,\n",
       " 3.7101154055304586,\n",
       " 0.44955044955004064]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_k_nearest_baboons_velocities(1, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To get the historic velocities per baboon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(12.85046947525376, 21.0),\n",
       " (12.97826878451604, 22.0),\n",
       " (12.644369496333105, 23.0),\n",
       " (12.850469475230561, 24.0),\n",
       " (12.85046947525376, 25.0),\n",
       " (12.85046947522877, 26.0),\n",
       " (12.850469475230561, 27.0),\n",
       " (12.850469475242162, 28.0),\n",
       " (12.978268784517812, 29.0),\n",
       " (12.723151376903115, 30.0),\n",
       " (12.85046947522877, 31.0),\n",
       " (12.978268784519587, 32.0),\n",
       " (12.850469475240375, 33.0),\n",
       " (12.723151376903115, 34.0),\n",
       " (12.978268784517812, 35.0),\n",
       " (12.85046947522877, 36.0),\n",
       " (12.723151376891556, 37.0),\n",
       " (12.85046947525376, 38.0),\n",
       " (12.97826878451604, 39.0),\n",
       " (12.850469475232348, 40.0),\n",
       " (12.723151376903115, 41.0),\n",
       " (12.978268784517812, 42.0),\n",
       " (12.850469475240375, 43.0),\n",
       " (12.850469475230561, 44.0),\n",
       " (12.850469475230561, 45.0),\n",
       " (12.850469475240375, 46.0),\n",
       " (12.85046947524395, 47.0),\n",
       " (12.85046947522877, 48.0),\n",
       " (12.850469475230561, 49.0),\n",
       " (12.850469475242162, 50.0),\n",
       " (50.51439438630578, 51.0),\n",
       " (50.56171446434248, 52.0),\n",
       " (50.51439438631872, 53.0),\n",
       " (50.41962099716843, 54.0),\n",
       " (50.41962099717059, 55.0),\n",
       " (50.51439438631872, 56.0),\n",
       " (50.56171446434248, 57.0),\n",
       " (50.51439438630578, 58.0),\n",
       " (50.56171446434248, 59.0),\n",
       " (50.56171446434033, 60.0),\n",
       " (22.147933257394644, 61.0),\n",
       " (22.43602563139897, 62.0),\n",
       " (22.436025631385856, 63.0),\n",
       " (22.4360256313999, 64.0),\n",
       " (22.1479332573937, 65.0),\n",
       " (22.436025631385856, 66.0),\n",
       " (22.436025631385856, 67.0),\n",
       " (22.436025631400828, 68.0),\n",
       " (22.147933257392754, 69.0),\n",
       " (22.436025631385856, 70.0),\n",
       " (11.564704124815332, 71.0),\n",
       " (11.722841487875636, 72.0),\n",
       " (11.436816325528085, 73.0),\n",
       " (11.722841487875636, 74.0),\n",
       " (11.436816325535226, 75.0),\n",
       " (11.722841487878512, 76.0),\n",
       " (11.564704124818244, 77.0),\n",
       " (11.596697750618631, 78.0),\n",
       " (11.564704124815332, 79.0),\n",
       " (11.596697750618631, 80.0),\n",
       " (11.564704124822395, 81.0),\n",
       " (11.722841487875636, 82.0),\n",
       " (11.436816325528085, 83.0),\n",
       " (11.722841487875636, 84.0),\n",
       " (11.436816325528085, 85.0),\n",
       " (11.722841487881384, 86.0),\n",
       " (11.564704124822395, 87.0),\n",
       " (11.596697750618631, 88.0),\n",
       " (11.564704124815332, 89.0),\n",
       " (11.596697750618631, 90.0),\n",
       " (18.354043731199216, 91.0),\n",
       " (18.354043731185968, 92.0),\n",
       " (18.17330821474484, 93.0),\n",
       " (18.499667088826754, 94.0),\n",
       " (18.20848896285627, 95.0),\n",
       " (18.465041119775933, 96.0),\n",
       " (18.20848896284303, 97.0),\n",
       " (18.499667088840003, 98.0),\n",
       " (18.17330821473157, 99.0),\n",
       " (18.20848896285627, 100.0),\n",
       " (45.29863701043879, 101.0),\n",
       " (45.29863701043879, 102.0),\n",
       " (45.44488375871215, 103.0),\n",
       " (45.29863701043844, 104.0),\n",
       " (45.26615624473773, 105.0),\n",
       " (45.26615624473107, 106.0),\n",
       " (45.29863701043844, 107.0),\n",
       " (45.298637010442114, 108.0),\n",
       " (45.44488375871215, 109.0),\n",
       " (45.298637010435456, 110.0),\n",
       " (0.0, 111.0),\n",
       " (0.0, 112.0),\n",
       " (0.0, 113.0),\n",
       " (0.0, 114.0),\n",
       " (0.0, 115.0),\n",
       " (0.0, 116.0),\n",
       " (0.0, 117.0),\n",
       " (0.0, 118.0),\n",
       " (0.0, 119.0),\n",
       " (0.0, 120.0),\n",
       " (0.0, 121.0),\n",
       " (0.0, 122.0),\n",
       " (0.0, 123.0),\n",
       " (0.0, 124.0),\n",
       " (0.0, 125.0),\n",
       " (0.0, 126.0),\n",
       " (0.0, 127.0),\n",
       " (0.0, 128.0),\n",
       " (0.0, 129.0),\n",
       " (0.0, 130.0)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO : Must check for 'extended' periods of discountinuous frames for future potentially sparese labeled data\n",
    "#TODO : Assuming data is ordered by frame (increasing timesteps/frames)\n",
    "#TODO : Warning : if there are not enough K historic frames then we pre pad with negative ones\n",
    "\n",
    "baboons_to_velocities = {} #maps the baboon's id to it's velocties and respective frames in a tuple\n",
    "\n",
    "for idx, row in training_data.iterrows():\n",
    "    current_frame = row.loc['frame']\n",
    "    current_position = (row.loc['centroid_x'], row.loc['centroid_y'])\n",
    "    current_velocity = row.loc['velocity']\n",
    "    current_id = row.loc['baboon id']\n",
    "    \n",
    "    if current_id not in baboons_to_velocities:\n",
    "        baboons_to_velocities[current_id] = []\n",
    "        \n",
    "    baboons_to_velocities[current_id].append((current_velocity, current_frame))\n",
    "\n",
    "\"\"\"\n",
    "Given a frame returns the k past velocities of the given baboon_id\n",
    "\n",
    "Pre pads with negative ones if there are less than k previous frames available in the labeled data\n",
    "\"\"\"\n",
    "def get_k_past_velocities(current_frame, baboon_id):\n",
    "    velocities = baboons_to_velocities[baboon_id]\n",
    "    \n",
    "    #annoying but I don't know how to do this easier\n",
    "    frame_index = -1\n",
    "    for idx, frame in enumerate(velocities):\n",
    "        if frame[1] == current_frame:\n",
    "            frame_index = idx\n",
    "            break\n",
    "            \n",
    "    if frame_index == -1:\n",
    "        raise RuntimeError('Frame does not exist in dataset for this baboon_id')\n",
    "        \n",
    "    if frame_index < config['k_historic_velocities']:\n",
    "        padded_output = list(np.ones(config['k_historic_velocities'] - frame_index) * -1)   \n",
    "        padded_output.extend([item[0] for item in velocities[:frame_index]])\n",
    "        return padded_output\n",
    "    \n",
    "    else:\n",
    "        return [item[0] for item in velocities[frame_index-config['k_historic_velocities'] : frame_index]]\n",
    "    \n",
    "baboons_to_velocities[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1.0, -1.0, -1.0, 12.85046947525376]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_k_past_velocities(22, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Data and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  3.76458762,  14.74763297,  35.8249973 ,  83.11665914,\n",
       "       407.57334622])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO : way too hard coded here. Make better\n",
    "\n",
    "training_data = pd.read_csv(config['input_training_data'])\n",
    "\n",
    "kmeans = pickle.load(open(config['kmeans_model_path'], 'rb'))\n",
    "kmeans_centers_sorted = np.sort(np.array(kmeans.cluster_centers_).squeeze())\n",
    "config['output_dimension'] = kmeans_centers_sorted.size + 1\n",
    "\n",
    "# test_predict = np.array([10])\n",
    "# print(kmeans.predict(test_predict.reshape(-1,1)))\n",
    "#TODO : add the sitting still !!!\n",
    "label_table = {}\n",
    "label_table[0] = [1,0,0,0,0,0] #sitting\n",
    "label_table[kmeans_centers_sorted[0]] = [0,1,0,0,0,0]\n",
    "label_table[kmeans_centers_sorted[1]] = [0,0,1,0,0,0]\n",
    "label_table[kmeans_centers_sorted[2]] = [0,0,0,1,0,0]\n",
    "label_table[kmeans_centers_sorted[3]] = [0,0,0,0,1,0]\n",
    "label_table[kmeans_centers_sorted[4]] = [0,0,0,0,0,1]\n",
    "\n",
    "\n",
    "kmeans_centers_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12.85046948, 53.82624892, 27.9806787 ,  1.47585282,  1.52817768,\n",
       "       -1.        , -1.        , -1.        , -1.        ])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# [ target v, k nearest baboon's v, k' past velocities ]\n",
    "# X = np.zeros((training_data.shape[0], config['input_dimension']))\n",
    "# labels = np.zeros(())\n",
    "X = []\n",
    "labels = []\n",
    "\n",
    "start_historic = 1 + config['k_nearest_baboons']\n",
    "\n",
    "\n",
    "for idx, row in training_data.iterrows():\n",
    "    current_frame = row.loc['frame']\n",
    "    current_position = (row.loc['centroid_x'], row.loc['centroid_y'])\n",
    "    current_velocity = row.loc['velocity']\n",
    "    current_id = row.loc['baboon id']\n",
    "    \n",
    "    current_label = training_data.loc[(training_data['frame'] == (current_frame + 1)) & (training_data['baboon id'] == current_id)]\n",
    "    if(current_label.empty):\n",
    "        #last frame for baboon, therefore no label\n",
    "        continue\n",
    "        \n",
    "    #create the label\n",
    "    current_one_hot = []\n",
    "    current_label_velocity = current_label['velocity'].to_numpy()\n",
    "    if current_label_velocity[0] == 0:\n",
    "        current_one_hot = label_table[0]\n",
    "    else:\n",
    "        kmeans_label = kmeans.predict(current_label_velocity.reshape(-1,1))[0]\n",
    "        current_one_hot = label_table[ kmeans.cluster_centers_[kmeans_label][0] ]\n",
    "    current_one_hot = np.array(current_one_hot)\n",
    "    \n",
    "    #create the datapoint\n",
    "    current_datapoint = np.zeros(config['input_dimension'])\n",
    "    current_datapoint[0] = current_velocity\n",
    "    current_datapoint[1:start_historic] = get_k_nearest_baboons_velocities(current_frame, current_id)\n",
    "    current_datapoint[start_historic:] = get_k_past_velocities(current_frame, current_id)\n",
    "    \n",
    "    X.append(current_datapoint)\n",
    "    labels.append(current_one_hot)\n",
    "\n",
    "\n",
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((52990, 9), (52990, 6))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array(X)\n",
    "labels = np.array(labels)\n",
    "\n",
    "X.shape, labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA NOT supported\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Nnet(\n",
       "  (input_layer): Sequential(\n",
       "    (0): Linear(in_features=9, out_features=500, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (main): Sequential(\n",
       "    (0): Linear(in_features=500, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=25088, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Dropout(p=0.5, inplace=False)\n",
       "    (9): Linear(in_features=4096, out_features=500, bias=True)\n",
       "    (10): ReLU(inplace=True)\n",
       "    (11): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (output): Linear(in_features=500, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO : add weighted classes to save from an unbalanced dataset\n",
    "# Check if your system supports CUDA\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "# Setup GPU optimization if CUDA is supported\n",
    "if use_cuda:\n",
    "    computing_device = torch.device(\"cuda\")\n",
    "    extras = {\"num_workers\": 1, \"pin_memory\": True}\n",
    "    print(\"CUDA is supported\")\n",
    "else: # Otherwise, train on the CPU\n",
    "    computing_device = torch.device(\"cpu\")\n",
    "    extras = False\n",
    "    print(\"CUDA NOT supported\")\n",
    "    \n",
    " \n",
    "if not config['checkpoint_training']:\n",
    "    net = Nnet(config['input_dimension'], config['output_dimension']).to(computing_device)\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(),lr = config['learning_rate'])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = loader(X, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = config['batch_size']\n",
    "validation_split = config['val_split']\n",
    "shuffle_dataset = config['shuffle_dataset']\n",
    "random_seed= 69 \n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, \n",
    "                                               sampler=train_sampler)\n",
    "    \n",
    "validation_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                                sampler=valid_sampler)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN!1!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started training epoch : 0\n",
      "Minibatch 0 loss : 0.5800476670265198\n",
      "Minibatch 1 loss : 0.4719441533088684\n",
      "Minibatch 2 loss : 0.4851217269897461\n",
      "Minibatch 3 loss : 1.1768085956573486\n",
      "Minibatch 4 loss : 0.5792779922485352\n",
      "Minibatch 5 loss : 0.3689684271812439\n",
      "Minibatch 6 loss : 0.7100775241851807\n",
      "Minibatch 7 loss : 0.6991098523139954\n",
      "Minibatch 8 loss : 1.085209608078003\n",
      "Minibatch 9 loss : 0.3221856951713562\n",
      "Minibatch 10 loss : 0.7733945250511169\n",
      "Minibatch 11 loss : 0.5279470682144165\n",
      "Minibatch 12 loss : 0.307323157787323\n",
      "Minibatch 13 loss : 0.49710744619369507\n",
      "Minibatch 14 loss : 0.7257239818572998\n",
      "Minibatch 15 loss : 0.4586210250854492\n",
      "Minibatch 16 loss : 0.47458377480506897\n",
      "Minibatch 17 loss : 0.6399723887443542\n",
      "Minibatch 18 loss : 0.310200035572052\n",
      "Minibatch 19 loss : 0.7783452272415161\n",
      "Minibatch 20 loss : 0.47781839966773987\n",
      "Minibatch 21 loss : 0.7255041599273682\n",
      "Minibatch 22 loss : 0.3601127564907074\n",
      "Minibatch 23 loss : 0.5080505609512329\n",
      "Minibatch 24 loss : 1.7527594566345215\n",
      "Minibatch 25 loss : 0.381939560174942\n",
      "Minibatch 26 loss : 0.478908896446228\n",
      "Minibatch 27 loss : 0.6771794557571411\n",
      "Minibatch 28 loss : 0.4183613061904907\n",
      "Minibatch 29 loss : 0.7082062363624573\n",
      "Minibatch 30 loss : 0.5787190198898315\n",
      "Minibatch 31 loss : 0.4334961175918579\n",
      "Minibatch 32 loss : 1.244372844696045\n",
      "Minibatch 33 loss : 0.6468930244445801\n",
      "Minibatch 34 loss : 0.5892461538314819\n",
      "Minibatch 35 loss : 0.45293352007865906\n",
      "Minibatch 36 loss : 0.7585873007774353\n",
      "Minibatch 37 loss : 0.45193251967430115\n",
      "Minibatch 38 loss : 0.5208256840705872\n",
      "Minibatch 39 loss : 0.7477090358734131\n",
      "Minibatch 40 loss : 0.6652827858924866\n",
      "Minibatch 41 loss : 0.8953191041946411\n",
      "Minibatch 42 loss : 0.6677836775779724\n",
      "Minibatch 43 loss : 0.5054005980491638\n",
      "Minibatch 44 loss : 0.6530317664146423\n",
      "Minibatch 45 loss : 0.5941755175590515\n",
      "Minibatch 46 loss : 0.6357078552246094\n",
      "Minibatch 47 loss : 0.5273440480232239\n",
      "Minibatch 48 loss : 0.516889214515686\n",
      "Minibatch 49 loss : 0.5207136869430542\n",
      "Epoch 1, average minibatch 50 loss: 0.6213434934616089\n",
      "Minibatch 50 loss : 0.8701925277709961\n",
      "Minibatch 51 loss : 0.569002628326416\n",
      "Minibatch 52 loss : 0.4458993673324585\n",
      "Minibatch 53 loss : 0.24857009947299957\n",
      "Minibatch 54 loss : 0.8499717116355896\n",
      "Minibatch 55 loss : 0.6215348839759827\n",
      "Minibatch 56 loss : 0.45196476578712463\n",
      "Minibatch 57 loss : 0.6292175054550171\n",
      "Minibatch 58 loss : 1.2331730127334595\n",
      "Minibatch 59 loss : 1.798945665359497\n",
      "Minibatch 60 loss : 0.41266024112701416\n",
      "Minibatch 61 loss : 0.5474255681037903\n",
      "Minibatch 62 loss : 0.5661909580230713\n",
      "Minibatch 63 loss : 0.7460615038871765\n",
      "Minibatch 64 loss : 0.47702866792678833\n",
      "Minibatch 65 loss : 0.5110100507736206\n",
      "Minibatch 66 loss : 0.6843992471694946\n",
      "Minibatch 67 loss : 1.721930980682373\n",
      "Minibatch 68 loss : 0.3514229357242584\n",
      "Minibatch 69 loss : 0.6206157803535461\n",
      "Minibatch 70 loss : 0.5060888528823853\n",
      "Minibatch 71 loss : 0.5611381530761719\n",
      "Minibatch 72 loss : 0.7113872766494751\n",
      "Minibatch 73 loss : 0.28856027126312256\n",
      "Minibatch 74 loss : 0.9188835024833679\n",
      "Minibatch 75 loss : 0.31197288632392883\n",
      "Minibatch 76 loss : 0.3516586422920227\n",
      "Minibatch 77 loss : 0.35038965940475464\n",
      "Minibatch 78 loss : 0.3957807123661041\n",
      "Minibatch 79 loss : 0.5156330466270447\n",
      "Minibatch 80 loss : 0.9424691200256348\n",
      "Minibatch 81 loss : 0.7511615753173828\n",
      "Minibatch 82 loss : 0.20502804219722748\n",
      "Minibatch 83 loss : 0.4337388277053833\n",
      "Minibatch 84 loss : 0.944776713848114\n",
      "Minibatch 85 loss : 0.9623277187347412\n",
      "Minibatch 86 loss : 0.39306169748306274\n",
      "Minibatch 87 loss : 0.5850422382354736\n",
      "Minibatch 88 loss : 0.4218837022781372\n",
      "Minibatch 89 loss : 0.5335681438446045\n",
      "Minibatch 90 loss : 0.3493102192878723\n",
      "Minibatch 91 loss : 0.4671369791030884\n",
      "Minibatch 92 loss : 0.5257721543312073\n",
      "Minibatch 93 loss : 0.3862352967262268\n",
      "Minibatch 94 loss : 0.4183775782585144\n",
      "Minibatch 95 loss : 0.7102846503257751\n",
      "Minibatch 96 loss : 0.29482749104499817\n",
      "Minibatch 97 loss : 0.2760978937149048\n",
      "Minibatch 98 loss : 0.4725935757160187\n",
      "Minibatch 99 loss : 0.7932558655738831\n",
      "Epoch 1, average minibatch 100 loss: 0.6027133464813232\n",
      "Minibatch 100 loss : 0.7265171408653259\n",
      "Minibatch 101 loss : 0.7989664077758789\n",
      "Minibatch 102 loss : 0.36957743763923645\n",
      "Minibatch 103 loss : 2.0180935859680176\n",
      "Minibatch 104 loss : 0.5103796720504761\n",
      "Minibatch 105 loss : 0.6682344079017639\n",
      "Minibatch 106 loss : 1.522121787071228\n",
      "Minibatch 107 loss : 0.7763683199882507\n",
      "Minibatch 108 loss : 0.3493959307670593\n",
      "Minibatch 109 loss : 0.652733325958252\n",
      "Minibatch 110 loss : 0.5175081491470337\n",
      "Minibatch 111 loss : 0.40260547399520874\n",
      "Minibatch 112 loss : 0.42799198627471924\n",
      "Minibatch 113 loss : 0.2896486520767212\n",
      "Minibatch 114 loss : 0.4865637719631195\n",
      "Minibatch 115 loss : 0.3926396071910858\n",
      "Minibatch 116 loss : 0.6705693006515503\n",
      "Minibatch 117 loss : 0.6944611668586731\n",
      "Minibatch 118 loss : 0.43419981002807617\n",
      "Minibatch 119 loss : 0.6544840335845947\n",
      "Minibatch 120 loss : 0.3962252736091614\n",
      "Minibatch 121 loss : 0.7098523378372192\n",
      "Minibatch 122 loss : 0.33408695459365845\n",
      "Minibatch 123 loss : 0.2855522930622101\n",
      "Minibatch 124 loss : 1.3397419452667236\n",
      "Minibatch 125 loss : 0.6969556212425232\n",
      "Minibatch 126 loss : 0.1998768150806427\n",
      "Minibatch 127 loss : 1.330950140953064\n",
      "Minibatch 128 loss : 0.293362557888031\n",
      "Minibatch 129 loss : 0.30768871307373047\n",
      "Minibatch 130 loss : 0.439558744430542\n",
      "Minibatch 131 loss : 1.4145039319992065\n",
      "Minibatch 132 loss : 0.8234331011772156\n",
      "Minibatch 133 loss : 0.5133316516876221\n",
      "Minibatch 134 loss : 0.6447939872741699\n",
      "Minibatch 135 loss : 0.8014802932739258\n",
      "Minibatch 136 loss : 0.6895433664321899\n",
      "Minibatch 137 loss : 0.7692729830741882\n",
      "Minibatch 138 loss : 0.4777120053768158\n",
      "Minibatch 139 loss : 0.469108521938324\n",
      "Minibatch 140 loss : 0.5388141870498657\n",
      "Minibatch 141 loss : 0.49713611602783203\n",
      "Minibatch 142 loss : 1.0191396474838257\n",
      "Minibatch 143 loss : 0.6112807393074036\n",
      "Minibatch 144 loss : 0.37849152088165283\n",
      "Minibatch 145 loss : 0.39502835273742676\n",
      "Minibatch 146 loss : 0.5402716994285583\n",
      "Minibatch 147 loss : 0.738233745098114\n",
      "Minibatch 148 loss : 0.5880213379859924\n",
      "Minibatch 149 loss : 0.4379757344722748\n",
      "Epoch 1, average minibatch 150 loss: 0.6408896446228027\n",
      "Minibatch 150 loss : 0.4327104091644287\n",
      "Minibatch 151 loss : 0.3937186002731323\n",
      "Minibatch 152 loss : 1.0150481462478638\n",
      "Minibatch 153 loss : 0.7035223245620728\n",
      "Minibatch 154 loss : 0.34095048904418945\n",
      "Minibatch 155 loss : 1.1934562921524048\n",
      "Minibatch 156 loss : 0.6501834988594055\n",
      "Minibatch 157 loss : 0.3392237722873688\n",
      "Minibatch 158 loss : 0.35531193017959595\n",
      "Minibatch 159 loss : 0.4569218158721924\n",
      "Minibatch 160 loss : 0.45090365409851074\n",
      "Minibatch 161 loss : 0.5339369773864746\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-40ec79565a73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# Update the weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;31m# Add this iteration's loss to the total_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mtotal_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    109\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Track the loss across training\n",
    "total_loss = []\n",
    "avg_minibatch_loss = []\n",
    "num_times_incraesed = 0\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "N = 50\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#TODO: Add dropout! When we do we MUST use model.eval() for val or testing and model.train() for training\n",
    "prev_val_loss = float(\"inf\")\n",
    "\n",
    "for epoch in range(config['epochs']):    \n",
    "    old_net_weights = net.state_dict().copy()\n",
    "    old_optimizer = optimizer.state_dict().copy()\n",
    "    \n",
    "    print(f\"Started training epoch : {epoch}\" )\n",
    "    \n",
    "\n",
    "    N_minibatch_loss = 0.0\n",
    "    total_epoch_loss = 0.0\n",
    "    average_epoch_loss = 0.0\n",
    "    num_minibatches = 0\n",
    "    \n",
    "     \n",
    "    net.train() #turns dropout on\n",
    "\n",
    "    # Get the next minibatch of images, labels for training\n",
    "    for minibatch_count, (datapoints, labels) in enumerate(train_loader, 0):\n",
    "\n",
    "        num_minibatches += 1\n",
    "        # Zero out the stored gradient (buffer) from the previous iteration\n",
    "        optimizer.zero_grad()\n",
    "        # Put the minibatch data in CUDA Tensors and run on the GPU if supported\n",
    "        datapoints, labels = datapoints.to(computing_device), labels.to(computing_device)\n",
    "        # Perform the forward pass through the network and compute the loss\n",
    "        outputs = net(datapoints.float())\n",
    "        labels = torch.max(labels, 1)[1]\n",
    "        \n",
    "        #computing the CEL using the net and the labels\n",
    "        loss = criterion(outputs, labels)\n",
    "#         print(f'Minibatch {minibatch_count} loss : {loss.item()}')\n",
    "        \n",
    "        # Automagically compute the gradients and backpropagate the loss through the network\n",
    "        loss.backward()\n",
    "        total_epoch_loss += loss.item()\n",
    "\n",
    "        # Update the weights\n",
    "        optimizer.step()    \n",
    "        # Add this iteration's loss to the total_loss\n",
    "        total_loss.append(loss.item())\n",
    "        N_minibatch_loss += loss\n",
    "        \n",
    "               \n",
    "        \n",
    "        if minibatch_count % N == 49:\n",
    "            #Print the loss averaged over the last N mini-batches\n",
    "            N_minibatch_loss /= N\n",
    "            print(f'Epoch {epoch + 1}, average minibatch {minibatch_count+1} loss: {N_minibatch_loss}')\n",
    "            # Add the averaged loss over N minibatches and reset the counter\n",
    "            avg_minibatch_loss.append(N_minibatch_loss)\n",
    "            N_minibatch_loss = 0.0\n",
    "    \n",
    "    average_epoch_loss = total_epoch_loss / num_minibatches\n",
    "    print(f\"Finished {epoch+ 1} epochs of training with average {average_epoch_loss} loss.\" )\n",
    "    \n",
    "    N_minibatch_val_loss = 0\n",
    "    val_data_size = 0\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        net.eval() #turns dropout off\n",
    "        print(\"validation starting: \")\n",
    "\n",
    "        for minibatch_count, (datapoints, labels) in enumerate(validation_loader, 0):\n",
    "            val_data_size += 1\n",
    "            datapoints, labels = images.to(computing_device), labels.to(computing_device)\n",
    "            outputs = net(datapoints)\n",
    "            val_loss = criterion(outputs, labels).item()\n",
    "            N_minibatch_val_loss += val_loss\n",
    "\n",
    "        N_minibatch_val_loss /= val_data_size\n",
    "        print(f'Epoch {epoch + 1} average validation loss over {val_data_size} datapoints : {N_minibatch_val_loss}' )\n",
    "        \n",
    "        #early stopping\n",
    "        if config['early_stop']:\n",
    "            print(str(N_minibatch_val_loss) + ' vs' + str(prev_val_loss))\n",
    "\n",
    "            if num_times_incraesed >= config['early_stop_epoch']:\n",
    "                print('early stopping triggered')\n",
    "                break\n",
    "            if N_minibatch_val_loss > prev_val_loss:\n",
    "                print('keeping old weights')\n",
    "                num_times_incraesed += 1\n",
    "                net.load_state_dict(old_net_weights)\n",
    "                optimizer.load_state_dict(old_optimizer)\n",
    "            else : \n",
    "                print('val is less than previous')\n",
    "                num_times_incraesed = 0\n",
    "                prev_val_loss = N_minibatch_val_loss\n",
    "\n",
    "             \n",
    "    training_losses.append(average_epoch_loss)\n",
    "    validation_losses.append(N_minibatch_val_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([], [])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_losses, validation_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'cross entropy error')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "min_entries = min(len(training_losses), len(validation_losses))\n",
    "epochs_plots = []\n",
    "for i in range(min_entries):\n",
    "    epochs_plots.append(i+1)\n",
    "    \n",
    "val_plots = training_losses[:min_entries]\n",
    "train_plots = validation_losses[:min_entries]\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs_plots, val_plots, label='val loss')\n",
    "plt.plot(epochs_plots, train_plots, label='train loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('cross entropy error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
